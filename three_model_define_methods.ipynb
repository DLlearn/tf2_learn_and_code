{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras和tensorflow2.0提供了三种定义模型的方式：\n",
    "下几点新功能，帮助你用它写博客：\n",
    " 1. Sequential API\n",
    " 2. Functional API \n",
    " 3. Model subclassing\n",
    " \n",
    "这就需要我们了解如何使用这三种方法来定义模型，还要学会什么时候该用什么方法。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、三种模型定义方式学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Sequential API 学习\n",
    "\n",
    "序列模型是layer-by-layer的，它是最简单的定义模型的方法但是有几个不足：\n",
    "- 不能够共享某一层\n",
    "- 不能有多个分支\n",
    "- 不能多个输入\n",
    "- 不能多个输出\n",
    "\n",
    "这种结构的经典网络比如有：Lenet5,AlexNet,VGGNet.\n",
    "定义一个简单的Sequential模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入必备包\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开始定义模型\n",
    "def shallownet_sequential(width,height,depth,classes):\n",
    "    #channel last，用输入形状来初始化模型\n",
    "    model = Sequential()\n",
    "    inputshape=(height,width,depth)\n",
    "    model.add(Conv2D(32,(3,3),padding='same',input_shape=inputshape))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    \n",
    "    #softmax\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着用函数式API模型，函数式API有更强功能：\n",
    "- 定义更复杂的模型\n",
    "- 支持多输入多输出\n",
    "- 可以定义模型分支，比较inception block,resnet block\n",
    "- 方便layer共享\n",
    "\n",
    "另外，对于任意的Sequential模型，都可以用函数式编程的方式来实现。\n",
    "函数式模型示例有：\n",
    "- ResNet\n",
    "- GoogleNet/Inception\n",
    "- Xception\n",
    "- SqueezeNet\n",
    "\n",
    "下面定义一个mini的GoogleNet.原文参见[文档](https://arxiv.org/abs/1409.4842)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `comv_module`:卷积模块，进行卷积，batchnorm,relu的操作，定义模块，方便复用。\n",
    "2. `inception_module`: 包括两个卷积模块，卷积核以3*3和1*1实现，padding=same,两个通道的输出会被拼接起来。\n",
    "3. `downsample_module`: 作用是减小尺寸。有两个分支：一个是卷积下采样，3*3的卷积核，stride=2*2,padding='valid'。另一个下采样方法是最大值池化，pool_size=3*3,strides=2*2,padding=valid,最后的输出拼接起来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现以上三种模块，然后通过各种组合，形成模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_module(x,k,kx,ky,stride,chandim,padding=\"same\"):\n",
    "    #conv-bn-relu\n",
    "    x = Conv2D(k,(kx,ky),strides=stride,padding=padding)(x)\n",
    "    x = BatchNormalization(axis=chandim)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "def inception_module(x,num1x1,num3x3,chandim):\n",
    "    conv_1x1 = conv_module(x,num1x1,1,1,(1,1),chandim)\n",
    "    conv_3_3 = conv_module(x,num3x3,3,3,(1,1),chandim)\n",
    "    x = concatenate([conv_1x1,conv_3_3],axis=chandim)\n",
    "    return x\n",
    "def downsample_module(x,k,chandim):\n",
    "    #conv downsample and pool downsample\n",
    "    conv_3x3 = conv_module(x,k,3,3,(2,2),chandim,padding='valid')\n",
    "    pool = MaxPooling2D((3,3),strides=(2,2))(x)\n",
    "    x = concatenate([conv_3x3,pool],axis=chandim)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#然后定义整个MiniGoogLeNet\n",
    "def minigooglenet_functional(width,height,depth,classes):\n",
    "    inputshape=(height,width,depth)\n",
    "    chandim=-1\n",
    "    #define inputs and firse conv\n",
    "    inputs = Input(shape=inputshape)\n",
    "    x = conv_module(inputs,96,3,3,(1,1),chandim)\n",
    "    #def two inception and followed by a downsample\n",
    "    x = inception_module(x,32,32,chandim)\n",
    "    x = inception_module(x,32,48,chandim)\n",
    "    x = downsample_module(x,80,chandim)\n",
    "    #def four inception and one downsample\n",
    "    x = inception_module(x,112,48,chandim)\n",
    "    x = inception_module(x,96,64,chandim)\n",
    "    x = inception_module(x,80,80,chandim)\n",
    "    x = inception_module(x,48,96,chandim)\n",
    "    x = downsample_module(x,96,chandim)\n",
    "    #def two inception followed by global pool and dropout\n",
    "    x = inception_module(x,176,160,chandim)\n",
    "    x = inception_module(x,176,160,chandim)\n",
    "    x = AveragePooling2D((7,7))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #softmax\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(classes)(x)\n",
    "    x = Activation('softmax')(x)\n",
    "\n",
    "    #create the model\n",
    "    model = Model(inputs,x,name='MiniGoogLeNet')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = minigooglenet_functional(32,32,3,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三种方也是最后一种方法，子类方法subclassing方法。在keras中Model类做为基本的类，可以在些基础上，进行会任意个人设置，带来很强的自由。但同进带来的不足就是没用序列和函数定义模型使用起来简单。既然子类方法有些难，为什么还要用呢，因为这对一些研究人员很好，可以模型所以部分进行控制。以下将是用subclassing的方法来实现一个小的VGGNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniVGGNetModel(Model):\n",
    "    def __init__(self,classes,chandim=-1):\n",
    "        super().__init__()\n",
    "        #(conv+relu)*2+pool\n",
    "        self.conv1a = Conv2D(32,(3,3),padding='same')\n",
    "        self.act1a = Activation('relu')\n",
    "        self.bn1a = BatchNormalization(axis=chandim)\n",
    "        self.conv1b = Conv2D(32,(3,3),padding='same')\n",
    "        self.act1b = Activation('relu')\n",
    "        self.bn1b = BatchNormalization(axis=chandim)\n",
    "        self.pool1 = MaxPooling2D(pool_size=(2,2))\n",
    "        ##(conv+relu)*2+pool\n",
    "        self.conv2a = Conv2D(32,(3,3),padding='same')\n",
    "        self.act2a = Activation('relu')\n",
    "        self.bn2a = BatchNormalization(axis=chandim)\n",
    "        self.conv2b = Conv2D(32,(3,3),padding='same')\n",
    "        self.act2b = Activation('relu')\n",
    "        self.bn2b = BatchNormalization(axis=chandim)\n",
    "        self.pool2 = MaxPooling2D(pool_size=(2,2))\n",
    "        #fully-connected\n",
    "        self.flatten=Flatten()\n",
    "        self.dense3=Dense(512)\n",
    "        self.act3 = Activation('relu')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.do3 = Dropout(0.5)\n",
    "        #softmax classifier\n",
    "        self.dense4=Dense(classes)\n",
    "        self.softmax=Activation('softmax')\n",
    "    def call(self,inputs):\n",
    "        #build 1\n",
    "        x = self.conv1a(inputs)\n",
    "        x = self.act1a(x)\n",
    "        x = self.bn1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.act1b(x)\n",
    "        x = self.bn1b(x)\n",
    "        x = self.poo1(x)\n",
    "        #bulid 2\n",
    "        x = self.conv2a(x)\n",
    "        x = self.act2a(x)\n",
    "        x = self.bn2a(x)\n",
    "        x = self.conv2b(x)\n",
    "        x = self.act2b(x)\n",
    "        x = self.bn2b(x)\n",
    "        x = self.pool2(x)\n",
    "        #build fully connected\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.do3(x)\n",
    "        #build softmax\n",
    "        x = self.dense4(x)\n",
    "        x = self.softmax(x)\n",
    "        #return the constructed model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG模型通常用Sequential的方法就可以实现。我们把以上代码写到models.py中\n",
    "下面实现模型训练代码。把训练代码写到train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")#这个设置可以使matplotlib保存.png图到磁盘\n",
    "from models import MiniVGGNetModel\n",
    "from models import minigooglenet_functional\n",
    "from models import shallownet_sequential\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建命令行解析\n",
    "#有三种模型结构可以选；将模型结构画出来并保存\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-m\",\"--model\",type=str,default=\"sequential\",choices=[\"sequential\",\"functional\",\"subclass\"],help=\"type of models\")\n",
    "parser.add_argument(\"-p\",\"--plot\",type=str,required=True,help=\"path to output plot file\")\n",
    "args=vars(parser.parse_args())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "紧接着做三件事：\n",
    "1. 初始化训练所需要参数\n",
    "2. 准备训练数据\n",
    "3. 构建数据扩增"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize parameters\n",
    "init_lr =1e-2\n",
    "batch_size=128\n",
    "num_epochs=60\n",
    "#初始化cifar10的标签名\n",
    "labelnames=[\"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "#load cifar10数据\n",
    "(train_x,train_y),(test_x,test_y)=cifar10.load_data()\n",
    "train_x = train_x.astype(\"float32\")/255.0\n",
    "test_x = test_x.astype(\"float32\")/255.0\n",
    "#将labels从数转成向量（one-hot方式）\n",
    "lb =LabelBinarizer()\n",
    "train_y=lb.fit_transform(train_y)\n",
    "test_y=lb.transform(test_y)\n",
    "#构建数据扩增\n",
    "aug = ImageDataGenerator(rotation_range=18,zoom_range=0.15,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.15,\n",
    "                        horizontal_flip=True,fill_mode=\"nearest\")\n",
    "\n",
    "#开始生成模型\n",
    "if args[\"model\"] == \"sequential\":\n",
    "    # instantiate a Keras Sequential model\n",
    "    print(\" using sequential model...\")\n",
    "    model = shallownet_sequential(32, 32, 3, len(labelnames))\n",
    " \n",
    " # check to see if we are using a Keras Functional model\n",
    "elif args[\"model\"] == \"functional\":\n",
    "    # instantiate a Keras Functional model\n",
    "    print(\"using functional model...\")\n",
    "    model = minigooglenet_functional(32, 32, 3, len(labelnames))\n",
    " \n",
    " # check to see if we are using a Keras Model class\n",
    "elif args[\"model\"] == \"class\":\n",
    "    # instantiate a Keras Model sub-class model\n",
    "    print(\" using model sub-classing...\")\n",
    "    model = MiniVGGNetModel(len(labelnames))\n",
    "    \n",
    "#编译模型\n",
    "opt = SGD(lr=init_lr,momentum=0.9,decay=init_lr/num_epochs)\n",
    "model.compile(optimizer=opt,loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "print(\"start training ...\")\n",
    "H=model.fit_generator(aug.flow(train_x,train_y,batch_size=batch_size),\n",
    "                    validation_data=(test_x,test_y),\n",
    "                    steps_per_epoch = train_x.shape[0]//batch_size,\n",
    "                    epochs=num_epochs,\n",
    "                    verbose=1\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成之后还要进行测试，并把训练过程参数画出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试模型\n",
    "print(\"start to evaluating network...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "print(classification_report(test_y.argmax(axis=1),\n",
    "                           predictions.argmax(axis=1),target_names=labelnames))\n",
    "#画结果图\n",
    "N = np.arange(0,num_epochs)\n",
    "title = \"Training Loss and Accuracy on CIFAR-10({})\".format(\n",
    "            args[\"model\"])\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N,H.history[\"loss\"],label=\"train_loss\")\n",
    "plt.plot(N,H.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.plot(N,H.history[\"accuracy\"],label=\"train_acc\")\n",
    "plt.plot(N,H.history[\"val_accuracy\"],label=\"val_acc\")\n",
    "plt.title(title)\n",
    "plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对Sequential模型进行训练\n",
    "。。。\n",
    "2. 对Functional模型进行训练\n",
    "。。。\n",
    "3. 对Subclassing模型进行训练"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
