{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文将对比tf.data与tf.keras中keras读数据方式下那种速度快，具体有三点：\n",
    "1. tf.data与keras生成器读数据速度对比\n",
    "2. tf.data包装后的keras生成器与原始生成器速度对比\n",
    "3. model.fit 与 model.fit_generator分别使用以上数据的实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、三种数据读取方式对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1、准备本文所用数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "(train_x,train_y),(test_x,test_y) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)\n",
    "train_x=np.expand_dims(train_x,-1)# keras生成器读数据要求输入形状是rank=4\n",
    "test_x=np.expand_dims(test_x,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2、准备tf.data数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(256).prefetch(buffer_size=1000).repeat()#训练数据会一直重复读\n",
    "test_ds = test_ds.batch(256).prefetch(buffer_size=1000)#测试数据只读一遍，所以没有加repeat，也可以加repeat(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 28, 28, 1) (256,)\n",
      "<RepeatDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.uint8, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "#检查数据\n",
    "for data,label in test_ds.take(1):\n",
    "    pass\n",
    "print(data.shape,label.shape)\n",
    "np.testing.assert_array_almost_equal(data,test_x[:256,...])#不返回报错信息表示数据相等\n",
    "np.testing.assert_array_almost_equal(label,test_y[:256])\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3、keras生成器读数据方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = tf.keras.preprocessing.image.ImageDataGenerator()#不做任何数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_flow=gen.flow(train_x,train_y,batch_size=256,shuffle=True)#与tf.data中batch相同大小，并且shuffle\n",
    "test_flow=gen.flow(test_x,test_y,batch_size=256,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.image.numpy_array_iterator.NumpyArrayIterator object at 0x000001A031C178C8>\n"
     ]
    }
   ],
   "source": [
    "#检查数据\n",
    "data,label= next(test_flow)\n",
    "np.testing.assert_array_almost_equal(data,test_x[:256,...])#不返回报错信息表示数据相等\n",
    "np.testing.assert_array_almost_equal(label,test_y[:256])\n",
    "print(train_flow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4、tf.data包装keras生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "wrap_train_ds = tf.data.Dataset.from_generator(lambda:gen.flow(train_x,train_y,batch_size=256,shuffle=True),\n",
    "    output_types=(tf.uint8, tf.uint8),\n",
    "    output_shapes = ([None,28,28,1],[None])\n",
    ")\n",
    "wrap_test_ds = tf.data.Dataset.from_generator(lambda:gen.flow(test_x,test_y,batch_size=256,shuffle=False),\n",
    "    output_types=(tf.uint8, tf.uint8),\n",
    "    output_shapes = (tf.TensorShape([None,28,28,1]),tf.TensorShape([None]))#tf.TensorShape可以不用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 28, 28, 1) (256,)\n",
      "<DatasetV1Adapter shapes: ((None, 28, 28, 1), (None,)), types: (tf.uint8, tf.uint8)>\n"
     ]
    }
   ],
   "source": [
    "#检查数据\n",
    "for data,label in wrap_test_ds.take(1):\n",
    "    pass\n",
    "print(data.shape,label.shape)\n",
    "np.testing.assert_array_almost_equal(data,test_x[:256,...])#不返回报错信息表示数据相等\n",
    "np.testing.assert_array_almost_equal(label,test_y[:256])\n",
    "print(wrap_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5、有了三种数据类型开始比较速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_timeit_steps = 5000\n",
    "\n",
    "def timeit(ds, steps=default_timeit_steps):\n",
    "    start = time.time()\n",
    "    it = iter(ds)\n",
    "    for i in range(steps):\n",
    "        batch = next(it)\n",
    "        if i%50 == 0:\n",
    "            print('.',end='')\n",
    "    print()\n",
    "    end = time.time()\n",
    "\n",
    "    duration = end-start\n",
    "    print(\"{} batches: {} s\".format(steps, duration))\n",
    "    print(\"{:0.5f} samples/s\".format(256*steps/duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "5000 batches: 2.3002309799194336 s\n",
      "556465.85546 samples/s\n"
     ]
    }
   ],
   "source": [
    "timeit(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "5000 batches: 9.523353099822998 s\n",
      "134406.44136 samples/s\n"
     ]
    }
   ],
   "source": [
    "timeit(train_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "5000 batches: 11.126928567886353 s\n",
      "115036.23774 samples/s\n"
     ]
    }
   ],
   "source": [
    "timeit(wrap_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对比结论**\n",
    "显然tf.data是最快的，wrap后的生成器最慢，我们肯定是要用tf.data的。关于wrap后比原始keras读数据的方式慢的原因，可能是因为这个生成器有问题，具体不再深究，所以我们就直接用tf.data了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.对tf.data进行改进"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 使用AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    "train_ds = train_ds.shuffle(buffer_size=1000).batch(256).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
    "test_ds = test_ds.batch(256).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "5000 batches: 2.3489813804626465 s\n",
      "544917.04815 samples/s\n"
     ]
    }
   ],
   "source": [
    "timeit(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提升看不明显，在其它数据上有试，效果会比自己随意设定的快"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 使用cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=train_ds.cache().shuffle(buffer_size=1000).batch(256).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()\n",
    "test_ds = test_ds.batch(256).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "5000 batches: 1.4627950191497803 s\n",
      "875037.16053 samples/s\n"
     ]
    }
   ],
   "source": [
    "timeit(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这速度的提升是惊人的，这些操作能使读取速度得到提升，但是提升多少依数据类型和其它参数的改变而改变"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 map的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    "def transfer(value1,value2):\n",
    "    return value1,value2 #什么操作都不加，只是为了配合map来使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.cache().shuffle(buffer_size=1000).map(transfer, num_parallel_calls=tf.data.experimental.AUTOTUNE) \\\n",
    "                           .batch(256).prefetch(buffer_size=tf.data.experimental.AUTOTUNE).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "5000 batches: 9.22971773147583 s\n",
      "138682.46432 samples/s\n"
     ]
    }
   ],
   "source": [
    "timeit(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要对数据做处理时要要用到map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于对shuffle,cache,batch,map,prefeach,repeat的顺序，排列组合情部很多，产生数据是相同的，但在数据最后一部分不够一个batch size的情况下有些许不同，但对训练没太大影响，测试数据只要全部读取就好。关于速度的影响，推荐使用上边代码的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3、对第一节中三种数据分别训练模型（fit,fig_generator的使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28,1)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dropout(0.2),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1875 steps\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 1.4856 - accuracy: 0.7065\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5632 - accuracy: 0.7833\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.5069 - accuracy: 0.8000\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4743 - accuracy: 0.8127\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4544 - accuracy: 0.8211\n",
      "It took 33.07719683647156 seconds\n"
     ]
    }
   ],
   "source": [
    "model= get_model()\n",
    "start = time.time()\n",
    "model.fit(train_ds, \n",
    "         steps_per_epoch=train_x.shape[0]//32,\n",
    "         epochs=5)\n",
    "print(\"It took {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 36s 19ms/step - loss: 7.8844 - accuracy: 0.5093\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 34s 18ms/step - loss: 7.2095 - accuracy: 0.55170s - loss: 7.2\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 6.6551 - accuracy: 0.5862\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 6.4122 - accuracy: 0.6015\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 39s 21ms/step - loss: 6.3284 - accuracy: 0.6067\n",
      "It took 184.5577735900879 seconds\n"
     ]
    }
   ],
   "source": [
    "model= get_model()\n",
    "start = time.time()\n",
    "model.fit_generator(train_flow, \n",
    "                   steps_per_epoch=train_x.shape[0]//32,\n",
    "                   epochs=5)\n",
    "print(\"It took {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1875 steps\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 1.7270 - accuracy: 0.6645\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.5929 - accuracy: 0.7829\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.5159 - accuracy: 0.8131\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4788 - accuracy: 0.8262\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.4607 - accuracy: 0.8320\n",
      "It took 65.25635552406311 seconds\n"
     ]
    }
   ],
   "source": [
    "model= get_model()\n",
    "start = time.time()\n",
    "model.fit(wrap_train_ds, \n",
    "         steps_per_epoch=train_x.shape[0]//32,\n",
    "         epochs=5)\n",
    "print(\"It took {} seconds\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出，tf.data训练要更快，并且精度高一些（这个有点不太明白，后其文章会对fit,fit_generator做更多说明）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
