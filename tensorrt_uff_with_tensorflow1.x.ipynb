{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorrt配合tensorflow工作有两种方式：\n",
    "- 1、uff,当整个模型完全转换成uff时，所有运算完全可以通过tensorrt来加速。\n",
    "- 2、tf-trt,这种工作方式下，模型中即使有tensorrt不支持的运算，也可以使用tensorrt加速，这部分运算会由tensorflow代替，而其余部分则会由tensorrt加速。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 uff\n",
    "\n",
    "具体的流程是把模型转freeze 成pb格式，再接着转换成uff格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1对于tensorflow 1.x而言，保存pb模型如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.layers.python.layers import utils\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes = 10):    \n",
    "    inputs = tf.placeholder(dtype=tf.float32,shape=(None,28,28,1),name='input_node')\n",
    "    net = slim.conv2d(inputs, 64, 5, 1, scope = 'conv1')  \n",
    "    net = slim.max_pool2d(net, 2, 2, scope = 'pool1')  \n",
    "    net = slim.conv2d(net, 128, 5, 1, scope = 'conv2')  \n",
    "    net = slim.max_pool2d(net, 2, 2, scope = 'pool2')  \n",
    "\n",
    "    shape = net.get_shape().as_list()\n",
    "    net = tf.reshape(net, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    net = slim.fully_connected(net, 128, scope = 'fc1')\n",
    "    net = slim.fully_connected(net, 128, scope = 'fc2')\n",
    "    net = slim.fully_connected(net, num_classes, activation_fn = None, scope = 'logits')\n",
    "    probs = tf.nn.softmax(net,name='myoutputnode')\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test path is not exist! make it !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_out_tf1 = 'tf1_pb'\n",
    "if os.path.exists(model_out_tf1):\n",
    "    t=input(\"The path(%s) is exist,do you want to delete it and remake?(y/n):\\n\" %(test,)).strip()\n",
    "    while t not in ['y','n']:\n",
    "        t = input(\"your input is invalid,please input again(y/n):\\n\")\n",
    "    if t =='y':\n",
    "        shutil.rmtree(model_out_tf1)\n",
    "        os.makedirs(model_out_tf1)\n",
    "    else:\n",
    "        pass\n",
    "else:\n",
    "    print(\"The test path is not exist! make it !\\n\")\n",
    "    os.makedirs(model_out_tf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x7ef80f8898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Pooling2D.call of <tensorflow.python.layers.pooling.MaxPooling2D object at 0x7ef80f8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef80f89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef80f89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef80f89e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef80f89e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef8064978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef8064978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef8064978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef8064978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef1f75a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef1f75a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef1f75a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef1f75a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "node: input_node\n",
      "node: conv1/weights\n",
      "node: conv1/biases\n",
      "node: conv1/Conv2D\n",
      "node: conv1/BiasAdd\n",
      "node: conv1/Relu\n",
      "node: pool1/MaxPool\n",
      "node: conv2/weights\n",
      "node: conv2/biases\n",
      "node: conv2/Conv2D\n",
      "node: conv2/BiasAdd\n",
      "node: conv2/Relu\n",
      "node: pool2/MaxPool\n",
      "node: Reshape/shape\n",
      "node: Reshape\n",
      "node: fc1/weights\n",
      "node: fc1/biases\n",
      "node: fc1/MatMul\n",
      "node: fc1/BiasAdd\n",
      "node: fc1/Relu\n",
      "node: fc2/weights\n",
      "node: fc2/biases\n",
      "node: fc2/MatMul\n",
      "node: fc2/BiasAdd\n",
      "node: fc2/Relu\n",
      "node: logits/weights\n",
      "node: logits/biases\n",
      "node: logits/MatMul\n",
      "node: logits/BiasAdd\n",
      "node: myoutputnode\n"
     ]
    }
   ],
   "source": [
    "#系统的普通信息和警告信息不打印\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "with tf.Graph().as_default() as g:\n",
    "    #模型未经训练，模型参数为初始化值\n",
    "    probs = create_model()\n",
    "    saver=tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        saver.save(sess,model_out_tf1+'/model.ckpt')\n",
    "        graph = tf.get_default_graph()\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            input_graph_def,\n",
    "            ['myoutputnode']\n",
    "        )\n",
    "        outgraph=tf.graph_util.remove_training_nodes(output_graph_def)\n",
    "        for node in outgraph.node:\n",
    "            print('node:',node.name)\n",
    "        tf.train.write_graph(outgraph,model_out_tf1,'frozen_model_tf1.pb',as_text=False)\n",
    "        tf.train.write_graph(outgraph,model_out_tf1,'frozen_model_tf1.pbtxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2对于tensorflow 1x 中的keras而言，保存pb模型如下：\n",
    "更多tensorflow1.x(keras)模型转pb,参考：https://blog.csdn.net/u011119817/article/details/103264080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset():\n",
    "    # Import the data\n",
    "    (x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Reshape the data\n",
    "    NUM_TRAIN = 60000\n",
    "    NUM_TEST = 10000\n",
    "    x_train = np.reshape(x_train, (NUM_TRAIN, 28, 28, 1))\n",
    "    x_test = np.reshape(x_test, (NUM_TEST, 28, 28, 1))\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(input_shape=[28,28, 1],name=\"input_node\"))\n",
    "    model.add(tf.keras.layers.Conv2D(64,5,1,activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Conv2D(128,5,1,activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPool2D(2,2))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax,name='myoutputnode'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def save(model, filename):\n",
    "    # First freeze the graph and remove training nodes.\n",
    "    output_names = model.output.op.name\n",
    "    sess = tf.keras.backend.get_session()\n",
    "    frozen_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), [output_names])\n",
    "    frozen_graph = tf.graph_util.remove_training_nodes(frozen_graph)\n",
    "    # Save the model\n",
    "    #tf.train.write_graph(frozen_graph,'tf2_pb','a.pb',as_text=False) #这个也可以\n",
    "    with open(filename, \"wb\") as ofile:\n",
    "        ofile.write(frozen_graph.SerializeToString())\n",
    "\n",
    "def main(save_path):\n",
    "    #x_train, y_train, x_test, y_test = process_dataset()\n",
    "    model = create_model()\n",
    "    model.summary()\n",
    "    #不训练也可以\n",
    "    # Train the model on the data\n",
    "    #model.fit(x_train, y_train, epochs = 5, verbose = 1)\n",
    "    # Evaluate the model on test data\n",
    "    #model.evaluate(x_test, y_test)\n",
    "    tf.keras.models.save_model(model,'tf1_pb/test.h5') #保存模型后加以加载\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path(tf1_pb) is exist,do you want to delete it and remake?(y/n):\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "model_out_keras1 = 'tf1_pb'\n",
    "if os.path.exists(model_out_keras1):\n",
    "    t=input(\"The path(%s) is exist,do you want to delete it and remake?(y/n):\\n\" %(model_out_keras1,)).strip()\n",
    "    while t not in ['y','n']:\n",
    "        t = input(\"your input is invalid,please input again(y/n):\\n\")\n",
    "    if t =='y':\n",
    "        shutil.rmtree(model_out_keras1)\n",
    "        os.makedirs(model_out_keras1)\n",
    "    else:\n",
    "        pass\n",
    "else:\n",
    "    print(\"The test path is not exist! make it !\\n\")\n",
    "    os.makedirs(model_out_keras1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 64)        1664      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 128)         204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "myoutputnode (Dense)         (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 486,666\n",
      "Trainable params: 486,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "output_names ['myoutputnode/Softmax']\n",
      "len node1 189\n",
      "INFO:tensorflow:Froze 10 variables.\n",
      "INFO:tensorflow:Converted 10 variables to const ops.\n",
      "##################################################################\n",
      "node: input_node\n",
      "node: conv2d/kernel\n",
      "node: conv2d/bias\n",
      "node: conv2d/Conv2D\n",
      "node: conv2d/BiasAdd\n",
      "node: conv2d/Relu\n",
      "node: max_pooling2d/MaxPool\n",
      "node: conv2d_1/kernel\n",
      "node: conv2d_1/bias\n",
      "node: conv2d_1/Conv2D\n",
      "node: conv2d_1/BiasAdd\n",
      "node: conv2d_1/Relu\n",
      "node: max_pooling2d_1/MaxPool\n",
      "node: flatten/Shape\n",
      "node: flatten/strided_slice/stack\n",
      "node: flatten/strided_slice/stack_1\n",
      "node: flatten/strided_slice/stack_2\n",
      "node: flatten/strided_slice\n",
      "node: flatten/Reshape/shape/1\n",
      "node: flatten/Reshape/shape\n",
      "node: flatten/Reshape\n",
      "node: dense/kernel\n",
      "node: dense/bias\n",
      "node: dense/MatMul\n",
      "node: dense/BiasAdd\n",
      "node: dense/Relu\n",
      "node: dense_1/kernel\n",
      "node: dense_1/bias\n",
      "node: dense_1/MatMul\n",
      "node: dense_1/BiasAdd\n",
      "node: dense_1/Relu\n",
      "node: myoutputnode/kernel\n",
      "node: myoutputnode/bias\n",
      "node: myoutputnode/MatMul\n",
      "node: myoutputnode/BiasAdd\n",
      "node: myoutputnode/Softmax\n",
      "len node1 36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tf1_pb/frozen_model_keras1.pb'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "tf.keras.backend.clear_session()\n",
    "hdf5_model=main(model_out_keras1)\n",
    "def freeze_session(session,keep_var_names=None,output_names=None,clear_devices=True):\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "#         freeze_var_names = list(set(v.op.name for v in tf1.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "#         output_names += [v.op.name for v in tf1.global_variables()]\n",
    "        print(\"output_names\",output_names)\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "#         for node in input_graph_def.node:\n",
    "#             print('node:', node.name)\n",
    "        print(\"len node1\",len(input_graph_def.node))\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph =  tf.graph_util.convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names)\n",
    "        \n",
    "        outgraph = tf.graph_util.remove_training_nodes(frozen_graph)#去掉与推理无关的内容\n",
    "        print(\"##################################################################\")\n",
    "        for node in outgraph.node:\n",
    "            print('node:', node.name)\n",
    "        print(\"len node1\",len(outgraph.node))\n",
    "        return outgraph\n",
    "\n",
    "frozen_graph = freeze_session(tf.keras.backend.get_session(),output_names=[out.op.name for out in hdf5_model.outputs])\n",
    "tf.train.write_graph(frozen_graph, model_out_keras1, \"frozen_model_keras1.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 调用两个pb并分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(file_path):\n",
    "\n",
    "    with tf.gfile.GFile(file_path,'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        with tf.Graph().as_default() as graph:\n",
    "            tf.import_graph_def(graph_def,input_map = None,return_elements = None,name = \"\",op_dict = None,producer_op_list = None)\n",
    "            graph_nodes = [n for n in graph_def.node]\n",
    "            return graph,graph_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23464\r\n",
      "-rw-rw-r-- 1 nano1       77 2月  10 15:59 checkpoint\r\n",
      "-rw-rw-r-- 1 nano1  1950030 2月  11 10:43 frozen_model_keras1.pb\r\n",
      "-rw-rw-r-- 1 nano1  4111847 2月  10 15:59 frozen_model_tf1.pb\r\n",
      "-rw-rw-r-- 1 nano1 11837730 2月  10 15:59 frozen_model_tf1.pbtxt\r\n",
      "-rw-rw-r-- 1 nano1  4109352 2月  10 15:59 model.ckpt.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 nano1      464 2月  10 15:59 model.ckpt.index\r\n",
      "-rw-rw-r-- 1 nano1    24294 2月  10 15:59 model.ckpt.meta\r\n",
      "-rw-rw-r-- 1 nano1  1970344 2月  11 10:43 test.h5\r\n"
     ]
    }
   ],
   "source": [
    "ll tf1_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.random.rand(1,28,28,1)\n",
    "np.save(\"tf1_pb/test_data.npy\",data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph1,graph_nodes1=load_graph('tf1_pb/frozen_model_tf1.pb')\n",
    "graph2,graph_nodes2=load_graph('tf1_pb/frozen_model_keras1.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.10509919 0.10234036 0.09939048 0.1059031  0.09565801 0.10008987\n",
      " 0.10413399 0.09897614 0.10693712 0.08147174]\n"
     ]
    }
   ],
   "source": [
    "input1 = graph1.get_tensor_by_name('input_node:0')\n",
    "output1 = graph1.get_tensor_by_name('myoutputnode:0')\n",
    "\n",
    "with tf.Session(graph=graph1) as sess:\n",
    "    softmax_values = sess.run(output1, feed_dict = {input1: data})\n",
    "    \n",
    "    for i, probs in enumerate(softmax_values):\n",
    "        probs = np.squeeze(probs)\n",
    "        print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09139289 0.10160429 0.1018744  0.11472408 0.10025626 0.09705692\n",
      " 0.10135855 0.09361386 0.09708725 0.10103158]\n"
     ]
    }
   ],
   "source": [
    "input2 = graph2.get_tensor_by_name('input_node:0')\n",
    "output2 = graph2.get_tensor_by_name('myoutputnode/Softmax:0')\n",
    "\n",
    "with tf.Session(graph=graph2) as sess:\n",
    "    softmax_values = sess.run(output2, feed_dict = {input2: data})\n",
    "    \n",
    "    for i, probs in enumerate(softmax_values):\n",
    "        probs = np.squeeze(probs)\n",
    "        print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 将pb模型转换成uff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uff.version 0.6.3\n"
     ]
    }
   ],
   "source": [
    "import uff\n",
    "print(\"uff.version\",uff.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(filename,output_node):\n",
    "    output_filename = filename[:filename.rfind('.')]  + '.uff'\n",
    "    print('output filename:',output_filename)\n",
    "    trt_graph = uff.from_tensorflow_frozen_model(filename, output_nodes=[output_node])\n",
    "    print('Done')\n",
    "    print('Writing to disk...')\n",
    "    with open(output_filename, 'wb') as f:\n",
    "        f.write(trt_graph)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output filename: tf1_pb/frozen_model_tf1.uff\n",
      "NOTE: UFF has been tested with TensorFlow 1.12.0. Other versions are not guaranteed to work\n",
      "WARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.\n",
      "UFF Version 0.6.3\n",
      "=== Automatically deduced input nodes ===\n",
      "[name: \"input_node\"\n",
      "op: \"Placeholder\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "=========================================\n",
      "\n",
      "Using output node myoutputnode\n",
      "Converting to UFF graph\n",
      "No. nodes: 31\n",
      "Done\n",
      "Writing to disk...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "convert('tf1_pb/frozen_model_tf1.pb','myoutputnode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output filename: tf1_pb/frozen_model_keras1.uff\n",
      "NOTE: UFF has been tested with TensorFlow 1.12.0. Other versions are not guaranteed to work\n",
      "WARNING: The version of TensorFlow installed on this system is not guaranteed to work with UFF.\n",
      "UFF Version 0.6.3\n",
      "=== Automatically deduced input nodes ===\n",
      "[name: \"input_node\"\n",
      "op: \"Placeholder\"\n",
      "attr {\n",
      "  key: \"dtype\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"shape\"\n",
      "  value {\n",
      "    shape {\n",
      "      dim {\n",
      "        size: -1\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 28\n",
      "      }\n",
      "      dim {\n",
      "        size: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "]\n",
      "=========================================\n",
      "\n",
      "Using output node myoutputnode/Softmax\n",
      "Converting to UFF graph\n",
      "DEBUG: convert reshape to flatten node\n",
      "No. nodes: 30\n",
      "Done\n",
      "Writing to disk...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "convert('tf1_pb/frozen_model_keras1.pb','myoutputnode/Softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 29400\r\n",
      "-rw-rw-r-- 1 nano1       77 2月  10 15:59 checkpoint\r\n",
      "-rw-rw-r-- 1 nano1  1950030 2月  11 10:43 frozen_model_keras1.pb\r\n",
      "-rw-rw-r-- 1 nano1  1949875 2月  11 11:24 frozen_model_keras1.uff\r\n",
      "-rw-rw-r-- 1 nano1  4111847 2月  10 15:59 frozen_model_tf1.pb\r\n",
      "-rw-rw-r-- 1 nano1 11837730 2月  10 15:59 frozen_model_tf1.pbtxt\r\n",
      "-rw-rw-r-- 1 nano1  4112546 2月  11 11:22 frozen_model_tf1.uff\r\n",
      "-rw-rw-r-- 1 nano1  4109352 2月  10 15:59 model.ckpt.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 nano1      464 2月  10 15:59 model.ckpt.index\r\n",
      "-rw-rw-r-- 1 nano1    24294 2月  10 15:59 model.ckpt.meta\r\n",
      "-rw-rw-r-- 1 nano1     6400 2月  11 10:50 test_data.npy\r\n",
      "-rw-rw-r-- 1 nano1  1970344 2月  11 10:43 test.h5\r\n"
     ]
    }
   ],
   "source": [
    "ll tf1_pb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 调用uff模型\n",
    "可以重新启动一下，把其它无关进程关闭来测试这个模型效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "# This import causes pycuda to automatically manage CUDA context creation and cleanup.\n",
    "import pycuda.autoinit\n",
    "\n",
    "import tensorrt as trt\n",
    "\n",
    "import sys, os\n",
    "\n",
    "\n",
    "# You can set the logger severity higher to suppress messages (or lower to display more messages).\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行第一个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 29404\r\n",
      "-rw-rw-r-- 1 nano1       77 2月  10 15:59 checkpoint\r\n",
      "-rw-rw-r-- 1 nano1  1950030 2月  11 10:43 frozen_model_keras1.pb\r\n",
      "-rw-rw-r-- 1 nano1  1949875 2月  11 11:24 frozen_model_keras1.uff\r\n",
      "-rw-rw-r-- 1 nano1  4111847 2月  10 15:59 frozen_model_tf1.pb\r\n",
      "-rw-rw-r-- 1 nano1 11837730 2月  10 15:59 frozen_model_tf1.pbtxt\r\n",
      "-rw-rw-r-- 1 nano1  4112546 2月  11 11:22 frozen_model_tf1.uff\r\n",
      "-rw-rw-r-- 1 nano1      617 2月  12 18:32 infer.py\r\n",
      "-rw-rw-r-- 1 nano1  4109352 2月  10 15:59 model.ckpt.data-00000-of-00001\r\n",
      "-rw-rw-r-- 1 nano1      464 2月  10 15:59 model.ckpt.index\r\n",
      "-rw-rw-r-- 1 nano1    24294 2月  10 15:59 model.ckpt.meta\r\n",
      "-rw-rw-r-- 1 nano1     6400 2月  11 10:50 test_data.npy\r\n",
      "-rw-rw-r-- 1 nano1  1970344 2月  11 10:43 test.h5\r\n"
     ]
    }
   ],
   "source": [
    "ll tf1_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'tf1_pb/frozen_model_tf1.uff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiB(val):\n",
    "    return val * 1 << 30\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "def allocate_buffers(engine):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    bindings = []\n",
    "    stream = cuda.Stream()\n",
    "    for binding in engine:\n",
    "        size = trt.volume(engine.get_binding_shape(binding)) * engine.max_batch_size\n",
    "        print('size:',size)\n",
    "        print('max_batch_size:',engine.max_batch_size)\n",
    "        dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "        # Allocate host and device buffers\n",
    "        host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "        device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "        # Append the device buffer to device bindings.\n",
    "        bindings.append(int(device_mem))\n",
    "        # Append to the appropriate list.\n",
    "        if engine.binding_is_input(binding):\n",
    "            inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        else:\n",
    "            outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    return inputs, outputs, bindings, stream\n",
    "\n",
    "def build_engine(model_file):\n",
    "    # For more information on TRT basics, refer to the introductory samples.\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "        builder.max_workspace_size = GiB(1)\n",
    "        # Parse the Uff Network\n",
    "        parser.register_input(ModelData.INPUT_NAME, ModelData.INPUT_SHAPE)\n",
    "        parser.register_output(ModelData.OUTPUT_NAME)\n",
    "        parser.parse(model_file, network)\n",
    "        # Build and return an engine.\n",
    "        return builder.build_cuda_engine(network)\n",
    "# Loads a test case into the provided pagelocked_buffer.\n",
    "def load_test_case(pagelocked_buffer):\n",
    "    # Flatten the image into a 1D array, normalize, and copy to pagelocked memory.\n",
    "    img = np.load('tf1_pb/test_data.npy').ravel()\n",
    "    np.copyto(pagelocked_buffer, img)\n",
    "    \n",
    "# This function is generalized for multiple inputs/outputs.\n",
    "# inputs and outputs are expected to be lists of HostDeviceMem objects.\n",
    "def do_inference(context, bindings, inputs, outputs, stream, batch_size=1):\n",
    "    # Transfer input data to the GPU.\n",
    "    [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in inputs]\n",
    "    # Run inference.\n",
    "    context.execute_async(batch_size=batch_size, bindings=bindings, stream_handle=stream.handle)\n",
    "    # Transfer predictions back from the GPU.\n",
    "    [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in outputs]\n",
    "    # Synchronize the stream\n",
    "    stream.synchronize()\n",
    "    # Return only the host outputs.\n",
    "    return [out.host for out in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelData(object):\n",
    "    MODEL_FILE = model_file\n",
    "    INPUT_NAME =\"input_node\"\n",
    "    INPUT_SHAPE = (1, 28, 28)\n",
    "    OUTPUT_NAME = \"myoutputnode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 784\n",
      "max_batch_size: 1\n",
      "size: 10\n",
      "max_batch_size: 1\n",
      "probs: [0.10509752 0.10234042 0.09939019 0.10590362 0.09565762 0.10009055\n",
      " 0.10413482 0.0989755  0.10693821 0.08147158]\n",
      "Prediction: 8\n"
     ]
    }
   ],
   "source": [
    "with build_engine(model_file) as engine:\n",
    "    # Build an engine, allocate buffers and create a stream.\n",
    "    # For more information on buffer allocation, refer to the introductory samples.\n",
    "    #pdb.set_trace()\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "    with engine.create_execution_context() as context:\n",
    "        load_test_case(pagelocked_buffer=inputs[0].host)\n",
    "        # For more information on performing inference, refer to the introductory samples.\n",
    "        # The common.do_inference function will return a list of outputs - we only have one in this case.\n",
    "        [output] = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        pred = np.argmax(output)\n",
    "        print(\"probs:\",output)\n",
    "        print(\"Prediction: \" + str(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 运行第二个模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file='tf1_pb/frozen_model_keras1.uff'\n",
    "class ModelData(object):\n",
    "    MODEL_FILE = model_file\n",
    "    INPUT_NAME =\"input_node\"\n",
    "    INPUT_SHAPE = (1, 28, 28)\n",
    "    OUTPUT_NAME = \"myoutputnode/Softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: 784\n",
      "max_batch_size: 1\n",
      "size: 10\n",
      "max_batch_size: 1\n",
      "probs: [0.09139289 0.10160428 0.1018744  0.11472408 0.10025626 0.09705692\n",
      " 0.10135854 0.09361386 0.09708725 0.10103157]\n",
      "Prediction: 3\n"
     ]
    }
   ],
   "source": [
    "with build_engine(model_file) as engine:\n",
    "    # Build an engine, allocate buffers and create a stream.\n",
    "    # For more information on buffer allocation, refer to the introductory samples.\n",
    "    #pdb.set_trace()\n",
    "    inputs, outputs, bindings, stream = allocate_buffers(engine)\n",
    "    with engine.create_execution_context() as context:\n",
    "        load_test_case(pagelocked_buffer=inputs[0].host)\n",
    "        # For more information on performing inference, refer to the introductory samples.\n",
    "        # The common.do_inference function will return a list of outputs - we only have one in this case.\n",
    "        [output] = do_inference(context, bindings=bindings, inputs=inputs, outputs=outputs, stream=stream)\n",
    "        pred = np.argmax(output)\n",
    "        print(\"probs:\",output)\n",
    "        print(\"Prediction: \" + str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
