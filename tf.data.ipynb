{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.data支持常用数据的读取，常用的api里有batch,zip,shuffle,prefetch,map,iterleave等,它们的排列组合使用情况很多，官方文档没有详细的说，看过tensorflow官方人员的讲解，但是只说了不同的顺序有不同的效果，具体什么效果他们也不好下结论。本文将做个小实验，来初步探索一下这个过程，目的是以后能正确使用，最基本的是来保证读入数据正确，如果能正确的基础上高效使用那是更好的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、顺序对结果的影响"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 产生实验用数据,并将数扰分加存到四个文件中，这样可以实现用tf.data来完成从多个文件中读取的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "from absl import logging\n",
    "import time\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0. 400. 401. 402. 403.]\n",
      " [  1. 404. 405. 406. 407.]\n",
      " [  2. 408. 409. 410. 411.]\n",
      " [  3. 412. 413. 414. 415.]\n",
      " [  4. 416. 417. 418. 419.]\n",
      " [  5. 420. 421. 422. 423.]\n",
      " [  6. 424. 425. 426. 427.]\n",
      " [  7. 428. 429. 430. 431.]\n",
      " [  8. 432. 433. 434. 435.]\n",
      " [  9. 436. 437. 438. 439.]]\n"
     ]
    }
   ],
   "source": [
    "datas = np.arange(400,2000).reshape(400,4) # 400个样本，每个样本有4个特征\n",
    "labels = np.arange(400).reshape(400,1) #每个样本的序号，方便分析数据变化\n",
    "ld = np.concatenate((labels,datas),axis=1).astype(np.float32)#把数据转换成float32\n",
    "print(ld[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着把数据保存成二进制文件，来适配tf.data实现从文件读数据的流程（而非内存）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath=\"/tmp/ourtest\"\n",
    "if not os.path.exists(savepath):\n",
    "    os.makedirs(savepath)#生成路保存数据的路径\n",
    "with open(\"/tmp/ourtest/test_0.dat\",\"wb\") as f:\n",
    "    f.write(ld[:100,:].tobytes())\n",
    "with open(\"/tmp/ourtest/test_1.dat\",\"wb\") as f:\n",
    "    f.write(ld[100:200,:].tobytes())\n",
    "with open(\"/tmp/ourtest/test_2.dat\",\"wb\") as f:\n",
    "    f.write(ld[200:300,:].tobytes())\n",
    "with open(\"/tmp/ourtest/test_3.dat\",\"wb\") as f:\n",
    "    f.write(ld[300:,:].tobytes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/ourtest\u001b[00m\r\n",
      "├── test_0.dat\r\n",
      "├── test_1.dat\r\n",
      "├── test_2.dat\r\n",
      "└── test_3.dat\r\n",
      "\r\n",
      "0 directories, 4 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/ourtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 实验一：理解map,batch,shuffle,repeat,prefetch的作用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "====================================after fixedlength read==========================================\n",
      "dataset type TensorSpec(shape=(), dtype=tf.string, name=None)\n",
      "one value of dataset tf.Tensor(b'\\x00\\x00\\x00\\x00\\x00\\x00\\xc8C\\x00\\x80\\xc8C\\x00\\x00\\xc9C\\x00\\x80\\xc9C', shape=(), dtype=string)\n",
      "====================================after parser==========================================\n",
      "dataset type (TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))\n",
      "one value of dataset [0.]\n",
      "====================================after batch read==========================================\n",
      "dataset type (TensorSpec(shape=(None, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.float32, name=None))\n",
      "one value of dataset [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "====================================after shuffle read==========================================\n",
      "dataset type (TensorSpec(shape=(None, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.float32, name=None))\n",
      "one value of dataset [200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "====================================after prefetch read==========================================\n",
      "dataset type (TensorSpec(shape=(None, None), dtype=tf.float32, name=None), TensorSpec(shape=(None, None), dtype=tf.float32, name=None))\n",
      "one value of dataset [200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n"
     ]
    }
   ],
   "source": [
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "print(\"====================================after fixedlength read==========================================\")\n",
    "print(\"dataset type\",dataset.element_spec)\n",
    "print(\"one value of dataset\",iter(dataset).next())#dataset是一个生成器，另外读数据方法是 for data in dataset: print data\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "print(\"====================================after parser==========================================\")\n",
    "print(\"dataset type\",dataset.element_spec)\n",
    "print(\"one value of dataset\",iter(dataset).next()[0].numpy().flatten())\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=False)\n",
    "print(\"====================================after batch read==========================================\")\n",
    "print(\"dataset type\",dataset.element_spec)\n",
    "print(\"one value of dataset\",iter(dataset).next()[0].numpy().flatten())\n",
    "dataset = dataset.shuffle(buffer_size=1000,reshuffle_each_iteration=False)\n",
    "print(\"====================================after shuffle read==========================================\")\n",
    "print(\"dataset type\",dataset.element_spec)\n",
    "print(\"one value of dataset\",iter(dataset).next()[0].numpy().flatten())\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "print(\"====================================after prefetch read==========================================\")\n",
    "print(\"dataset type\",dataset.element_spec)\n",
    "print(\"one value of dataset\",iter(dataset).next()[0].numpy().flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验二 1.2加或不加shuffle和prefetch以及调整顺序等地比实验"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 batch整除总数据量，无shuffle,prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.\n",
      " 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.\n",
      " 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "[100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
      " 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127.\n",
      " 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141.\n",
      " 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      "[150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163.\n",
      " 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177.\n",
      " 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      "[200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "[250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
      " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
      " 278. 279. 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
      " 292. 293. 294. 295. 296. 297. 298. 299.]\n",
      "[300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313.\n",
      " 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
      " 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349.]\n",
      "[350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399.]\n",
      "the average time is %s sec : 0.013164019584655762\n"
     ]
    }
   ],
   "source": [
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=False)\n",
    "#dataset = dataset.shuffle(buffer_size=1000,reshuffle_each_iteration=False)\n",
    "# dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 batch整除总数据量，无shuffle,有prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.\n",
      " 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.\n",
      " 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "[100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
      " 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127.\n",
      " 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141.\n",
      " 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      "[150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163.\n",
      " 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177.\n",
      " 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      "[200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "[250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
      " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
      " 278. 279. 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
      " 292. 293. 294. 295. 296. 297. 298. 299.]\n",
      "[300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313.\n",
      " 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
      " 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349.]\n",
      "[350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399.]\n",
      "the average time is %s sec : 0.014354395866394042\n"
     ]
    }
   ],
   "source": [
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=False)\n",
    "#dataset = dataset.shuffle(buffer_size=1000,reshuffle_each_iteration=False)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "            \n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 batch 不能整除总数据量，，drop_remainder=False，无shuffle,有prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n",
      "[ 51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.  64.\n",
      "  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.  78.\n",
      "  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.  92.\n",
      "  93.  94.  95.  96.  97.  98.  99. 100. 101.]\n",
      "[102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115.\n",
      " 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129.\n",
      " 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143.\n",
      " 144. 145. 146. 147. 148. 149. 150. 151. 152.]\n",
      "[153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166.\n",
      " 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180.\n",
      " 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194.\n",
      " 195. 196. 197. 198. 199. 200. 201. 202. 203.]\n",
      "[204. 205. 206. 207. 208. 209. 210. 211. 212. 213. 214. 215. 216. 217.\n",
      " 218. 219. 220. 221. 222. 223. 224. 225. 226. 227. 228. 229. 230. 231.\n",
      " 232. 233. 234. 235. 236. 237. 238. 239. 240. 241. 242. 243. 244. 245.\n",
      " 246. 247. 248. 249. 250. 251. 252. 253. 254.]\n",
      "[255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266. 267. 268.\n",
      " 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 282.\n",
      " 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294. 295. 296.\n",
      " 297. 298. 299. 300. 301. 302. 303. 304. 305.]\n",
      "[306. 307. 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319.\n",
      " 320. 321. 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333.\n",
      " 334. 335. 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347.\n",
      " 348. 349. 350. 351. 352. 353. 354. 355. 356.]\n",
      "[357. 358. 359. 360. 361. 362. 363. 364. 365. 366. 367. 368. 369. 370.\n",
      " 371. 372. 373. 374. 375. 376. 377. 378. 379. 380. 381. 382. 383. 384.\n",
      " 385. 386. 387. 388. 389. 390. 391. 392. 393. 394. 395. 396. 397. 398.\n",
      " 399.]\n",
      "the average time is %s sec : 0.0138258695602417\n"
     ]
    }
   ],
   "source": [
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=51\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=False)\n",
    "#dataset = dataset.shuffle(buffer_size=1000,reshuffle_each_iteration=False)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 batch 不能整除总数据量，，drop_remainder=True，无shuffle,有prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n",
      "[ 51.  52.  53.  54.  55.  56.  57.  58.  59.  60.  61.  62.  63.  64.\n",
      "  65.  66.  67.  68.  69.  70.  71.  72.  73.  74.  75.  76.  77.  78.\n",
      "  79.  80.  81.  82.  83.  84.  85.  86.  87.  88.  89.  90.  91.  92.\n",
      "  93.  94.  95.  96.  97.  98.  99. 100. 101.]\n",
      "[102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113. 114. 115.\n",
      " 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127. 128. 129.\n",
      " 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141. 142. 143.\n",
      " 144. 145. 146. 147. 148. 149. 150. 151. 152.]\n",
      "[153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166.\n",
      " 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180.\n",
      " 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194.\n",
      " 195. 196. 197. 198. 199. 200. 201. 202. 203.]\n",
      "[204. 205. 206. 207. 208. 209. 210. 211. 212. 213. 214. 215. 216. 217.\n",
      " 218. 219. 220. 221. 222. 223. 224. 225. 226. 227. 228. 229. 230. 231.\n",
      " 232. 233. 234. 235. 236. 237. 238. 239. 240. 241. 242. 243. 244. 245.\n",
      " 246. 247. 248. 249. 250. 251. 252. 253. 254.]\n",
      "[255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266. 267. 268.\n",
      " 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280. 281. 282.\n",
      " 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294. 295. 296.\n",
      " 297. 298. 299. 300. 301. 302. 303. 304. 305.]\n",
      "[306. 307. 308. 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319.\n",
      " 320. 321. 322. 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333.\n",
      " 334. 335. 336. 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347.\n",
      " 348. 349. 350. 351. 352. 353. 354. 355. 356.]\n",
      "the average time is %s sec : 0.013829517364501952\n"
     ]
    }
   ],
   "source": [
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=51\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=True)\n",
    "#dataset = dataset.shuffle(buffer_size=1000,reshuffle_each_iteration=False)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 batch 能整除总数据量，，drop_remainder=False，有shuffle=1、batch、2*batch，有prefetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.\n",
      " 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.\n",
      " 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "[100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
      " 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127.\n",
      " 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141.\n",
      " 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      "[150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163.\n",
      " 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177.\n",
      " 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      "[200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "[250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
      " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
      " 278. 279. 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
      " 292. 293. 294. 295. 296. 297. 298. 299.]\n",
      "[300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313.\n",
      " 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
      " 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349.]\n",
      "[350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399.]\n",
      "the average time is %s sec : 0.014731049537658691\n"
     ]
    }
   ],
   "source": [
    "#shuffle buffer_size=1\n",
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=True)\n",
    "dataset = dataset.shuffle(buffer_size=1,reshuffle_each_iteration=False)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "[250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
      " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
      " 278. 279. 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
      " 292. 293. 294. 295. 296. 297. 298. 299.]\n",
      "[350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399.]\n",
      "[100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
      " 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127.\n",
      " 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141.\n",
      " 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      "[150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163.\n",
      " 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177.\n",
      " 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313.\n",
      " 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
      " 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.\n",
      " 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.\n",
      " 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "the average time is %s sec : 0.016156482696533202\n"
     ]
    }
   ],
   "source": [
    "#shuffle buffer_size=batchsize\n",
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=True)\n",
    "dataset = dataset.shuffle(buffer_size=batchsize,reshuffle_each_iteration=False)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313.\n",
      " 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
      " 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349.]\n",
      "[100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
      " 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127.\n",
      " 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141.\n",
      " 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      "[350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399.]\n",
      "[250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
      " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
      " 278. 279. 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
      " 292. 293. 294. 295. 296. 297. 298. 299.]\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163.\n",
      " 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177.\n",
      " 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.\n",
      " 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.\n",
      " 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "[200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "the average time is %s sec : 0.01557457447052002\n"
     ]
    }
   ],
   "source": [
    "#shuffle buffer_size=2*batchsize\n",
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=True)\n",
    "dataset = dataset.shuffle(buffer_size=2*batchsize,reshuffle_each_iteration=False)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将shuffle放到batch前边，再次重复上边三种buffer_size的实验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
      " 36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49.]\n",
      "[50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67.\n",
      " 68. 69. 70. 71. 72. 73. 74. 75. 76. 77. 78. 79. 80. 81. 82. 83. 84. 85.\n",
      " 86. 87. 88. 89. 90. 91. 92. 93. 94. 95. 96. 97. 98. 99.]\n",
      "[100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112. 113.\n",
      " 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126. 127.\n",
      " 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140. 141.\n",
      " 142. 143. 144. 145. 146. 147. 148. 149.]\n",
      "[150. 151. 152. 153. 154. 155. 156. 157. 158. 159. 160. 161. 162. 163.\n",
      " 164. 165. 166. 167. 168. 169. 170. 171. 172. 173. 174. 175. 176. 177.\n",
      " 178. 179. 180. 181. 182. 183. 184. 185. 186. 187. 188. 189. 190. 191.\n",
      " 192. 193. 194. 195. 196. 197. 198. 199.]\n",
      "[200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210. 211. 212. 213.\n",
      " 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224. 225. 226. 227.\n",
      " 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238. 239. 240. 241.\n",
      " 242. 243. 244. 245. 246. 247. 248. 249.]\n",
      "[250. 251. 252. 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263.\n",
      " 264. 265. 266. 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277.\n",
      " 278. 279. 280. 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291.\n",
      " 292. 293. 294. 295. 296. 297. 298. 299.]\n",
      "[300. 301. 302. 303. 304. 305. 306. 307. 308. 309. 310. 311. 312. 313.\n",
      " 314. 315. 316. 317. 318. 319. 320. 321. 322. 323. 324. 325. 326. 327.\n",
      " 328. 329. 330. 331. 332. 333. 334. 335. 336. 337. 338. 339. 340. 341.\n",
      " 342. 343. 344. 345. 346. 347. 348. 349.]\n",
      "[350. 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363.\n",
      " 364. 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377.\n",
      " 378. 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391.\n",
      " 392. 393. 394. 395. 396. 397. 398. 399.]\n",
      "the average time is %s sec : 0.011891913414001466\n"
     ]
    }
   ],
   "source": [
    "#shuffle buffer_size=1\n",
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset = dataset.shuffle(buffer_size=1,reshuffle_each_iteration=False)\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=True)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data files: ['/tmp/ourtest/test_0.dat', '/tmp/ourtest/test_1.dat', '/tmp/ourtest/test_2.dat', '/tmp/ourtest/test_3.dat']\n",
      "[ 5. 22. 42. 38. 28. 16. 15. 48. 30. 23. 49. 12. 29. 19. 24.  7.  3.  6.\n",
      " 53.  2. 11. 55. 62. 31. 17. 27. 74. 73. 57. 56. 71. 66. 78. 67. 32. 63.\n",
      " 83. 43. 59. 58.  8. 89. 18. 10. 40. 50.  9. 81. 25. 41.]\n",
      "[ 26.  98.  45. 101.   1.  65.  79. 104.  93.  61. 108.  77. 105.  14.\n",
      "  36.  95.  85. 103.  75.  35.  91.  68.  37.  70.  90. 107.  46. 121.\n",
      "   4.  13. 112. 100.  82.  84. 102. 114. 133.  80.  52.  94.  99.  97.\n",
      " 141.   0. 120. 144. 106. 130. 123.  39.]\n",
      "[140. 111. 143.  51.  64. 148.  87. 149. 113. 134.  72.  76.  33. 128.\n",
      "  21. 153.  86. 151.  96.  69. 116. 150. 157. 145.  88.  60. 168. 166.\n",
      " 154. 131. 137. 163. 160. 158. 117.  92. 167. 186. 174.  34. 127. 138.\n",
      " 177. 164. 181. 115. 136.  47. 183. 195.]\n",
      "[124. 139. 146. 176. 165. 171. 197. 189. 175. 178. 199. 205. 125. 135.\n",
      " 173. 204.  44. 209. 187. 215. 190. 132. 155. 184. 218. 201. 110. 118.\n",
      " 179. 213. 222.  20. 223. 226. 221. 216. 225. 142. 161. 192. 159. 233.\n",
      " 211. 230. 214. 207. 219. 241. 200. 234.]\n",
      "[147. 169. 239. 238. 129. 224. 220. 252. 229. 243. 185. 227. 194. 251.\n",
      " 203. 246. 232. 256. 193. 257. 109. 122.  54. 268. 254. 242. 247. 266.\n",
      " 152. 119. 170. 228. 267. 188. 210. 248. 272. 258. 284. 285. 271. 262.\n",
      " 274. 180. 191. 270. 240. 237. 196. 276.]\n",
      "[294. 172. 265. 296. 303. 217. 231. 290. 236. 305. 244. 287. 299. 309.\n",
      " 235. 280. 315. 312. 275. 292. 291. 317. 310. 313. 318. 282. 301. 323.\n",
      " 300. 308. 325. 269. 250. 295. 162. 327. 297. 311. 324. 307. 279. 332.\n",
      " 331. 259. 206. 264. 314. 198. 255. 289.]\n",
      "[326. 253. 338. 277. 328. 208. 261. 333. 353. 339. 344. 359. 286. 357.\n",
      " 354. 361. 212. 322. 337. 329. 360. 330. 371. 260. 373. 341. 375. 335.\n",
      " 358. 320. 347. 367. 343. 369. 356. 383. 355. 281. 273. 321. 365. 381.\n",
      " 249. 202. 304. 336. 391. 378. 345. 334.]\n",
      "[389. 368. 293. 352. 386. 398. 387. 370. 245. 263. 362. 182. 399. 395.\n",
      " 288. 380. 379. 283. 397. 363. 298. 342. 340. 384. 372. 382. 278. 393.\n",
      " 302. 348. 392. 351. 390. 319. 374. 366. 156. 306. 394. 364. 126. 346.\n",
      " 350. 316. 376. 349. 388. 385. 377. 396.]\n",
      "the average time is %s sec : 0.012627458572387696\n"
     ]
    }
   ],
   "source": [
    "#shuffle buffer_size=batchsize\n",
    "filenames=sorted(tf.io.gfile.glob('/tmp/ourtest/test*')) #获取文件\n",
    "print(\"data files:\",filenames)\n",
    "dataset= tf.data.FixedLengthRecordDataset(filenames=filenames,record_bytes=5*np.dtype(np.float32).itemsize)#单个样本的总节节数\n",
    "def parser(value):\n",
    "    \"\"\"\n",
    "    需要对dataset中的数据进行解码\n",
    "    \"\"\"\n",
    "    record = tf.io.decode_raw(value,out_type=tf.float32)\n",
    "    label = record[:1]\n",
    "    data = record[1:]\n",
    "    return label,data\n",
    "#map可以叠加多个重复使用,第二个参数是用来加速的，可以使用tf.data.experimental.AUTOTUNE自适应的来做\n",
    "dataset=dataset.map(parser,num_parallel_calls=None)\n",
    "batchsize=50\n",
    "# 第二个参数表示数据不能被batch_size整除，剩下的数据小于batch_size后是否丢弃\n",
    "dataset = dataset.shuffle(buffer_size=batchsize,reshuffle_each_iteration=False)\n",
    "dataset=dataset.batch(batch_size=batchsize,drop_remainder=True)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)#这个参数可以指定也可以\n",
    "iterations=10\n",
    "total_time=[]\n",
    "values=[]\n",
    "for t in range(iterations):\n",
    "    start = time.time()\n",
    "    for i,(label,data) in enumerate(dataset):\n",
    "        if t==0:\n",
    "            print(label.numpy().flatten())\n",
    "            values.extend(label.numpy().flatten().tolist())\n",
    "    duration=time.time()-start\n",
    "    total_time.append(duration)\n",
    "print(\"the average time is %s sec :\",str(np.mean(total_time)))\n",
    "print(\"statistic of values\",Counter(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本部分实验结论\n",
    "- 1.2.1和1.2.2做对比，加prefetch后时间并没有显著提升，可能是数据太小。\n",
    "- 1.2.3和1.2.4做对比，可以看出drop_remainder可以把不足一个batch大小的数据选择舍弃，显然测试时这个参数一定是False,只能处理固定batch的数据的时候，这个参数要设置成True.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 batchsize 不整除总数据量（这种情况更加符合实际）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0  1\n",
       "row_0      \n",
       "0      0  1\n",
       "1      1  1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tianlianglllll1@foxmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
