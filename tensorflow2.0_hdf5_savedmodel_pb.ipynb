{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将把训练好的模型分别保存成hdf5和saved model格式，然后完成它们之间的互相转换以及分别转tensorflow1.x的pb格式，具体有：\n",
    "1. hdf5转saved model,并验证转换后的saved model与直接保存的saved model的无差异性（大小，精度）\n",
    "2. saved model转hdf5,并验证转换后的hdf5与直接保存的hdf5的无差异性（大小，精度）\n",
    "3. hdf5转pb,并验证转换后的pb与直接原始的的hdf5的无差异性（大小，精度）\n",
    "4. saved mode转pb,并验证转换后的pb与直接原始的的saved mode的无差异性（大小，精度）\n",
    "5. 对比hdf5所转pb与saved model所转pb的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.__version__\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(\"tf.__version__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model(/tmp/test/hdf5_model) saved path has exist,Do you want to delete and remake it?(y/n)y\n",
      "The model(/tmp/test/saved_model) saved path has exist,Do you want to delete and remake it?(y/n)y\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "epochs=6\n",
    "regularizer=1e-3\n",
    "total_train_samples=60000\n",
    "total_test_samples=10000\n",
    "\n",
    "output_folder=\"/tmp/test/hdf5_model\"\n",
    "output_folder1=\"/tmp/test/saved_model\"\n",
    "output_folder2=\"/tmp/test/pb_model\"\n",
    "for m in (output_folder,output_folder1,output_folder2):\n",
    "    if os.path.exists(m):\n",
    "        inc=input(\"The model(%s) saved path has exist,Do you want to delete and remake it?(y/n)\"%m)\n",
    "        while(inc.lower() not in ['y','n']):\n",
    "            inc=input(\"The model saved path has exist,Do you want to delete and remake it?(y/n)\")\n",
    "        if inc.lower()=='y':\n",
    "            shutil.rmtree(m)\n",
    "            os.makedirs(m)\n",
    "    elif not os.path.exists(m):\n",
    "        os.makedirs(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the available GPUs:\n",
      " [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#指定显卡\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')#列出所有可见显卡\n",
    "print(\"All the available GPUs:\\n\",physical_devices)\n",
    "if physical_devices:\n",
    "    gpu=physical_devices[0]#显示第一块显卡\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)#根据需要自动增长显存\n",
    "    tf.config.experimental.set_visible_devices(gpu, 'GPU')#只选择第一块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#准备数据\n",
    "fashion_mnist=tf.keras.datasets.fashion_mnist\n",
    "(train_x,train_y),(test_x,test_y)=fashion_mnist.load_data()\n",
    "\n",
    "train_x,test_x = train_x[...,np.newaxis]/255.0,test_x[...,np.newaxis]/255.0\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_x,train_y))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_x,test_y))\n",
    " \n",
    "train_ds=train_ds.shuffle(buffer_size=batch_size*10).batch(batch_size).prefetch(buffer_size = tf.data.experimental.AUTOTUNE).repeat()\n",
    "test_ds = test_ds.batch(batch_size).prefetch(buffer_size = tf.data.experimental.AUTOTUNE)#不加repeat，执行一次就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_node (InputLayer)      [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 28, 28, 128)       3328      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 256)       819456    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "output_node (Dense)          (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,429,834\n",
      "Trainable params: 2,429,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#定义模型\n",
    "l2 = tf.keras.regularizers.l2(regularizer)#定义模型正则化方法\n",
    "ini = tf.keras.initializers.he_normal()#定义参数初始化方法\n",
    "conv2d = partial(tf.keras.layers.Conv2D,activation='relu',padding='same',kernel_regularizer=l2,bias_regularizer=l2)\n",
    "fc = partial(tf.keras.layers.Dense,activation='relu',kernel_regularizer=l2,bias_regularizer=l2)\n",
    "maxpool=tf.keras.layers.MaxPooling2D\n",
    "dropout=tf.keras.layers.Dropout\n",
    "def test_model():\n",
    "    x_input = tf.keras.layers.Input(shape=(28,28,1),name='input_node')\n",
    "    x = conv2d(128,(5,5))(x_input)\n",
    "    x = maxpool((2,2))(x)\n",
    "    x = conv2d(256,(5,5))(x)\n",
    "    x = maxpool((2,2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = fc(128)(x)\n",
    "    x_output=fc(10,activation=None,name='output_node')(x)\n",
    "    model = tf.keras.models.Model(inputs=x_input,outputs=x_output) \n",
    "    return model\n",
    "model = test_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编译模型\n",
    "initial_learning_rate=0.01\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate,momentum=0.95)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics=['accuracy','sparse_categorical_crossentropy']\n",
    "model.compile(optimizer=optimizer,loss=loss,metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 937 steps, validate for 157 steps\n",
      "Epoch 1/6\n",
      "937/937 [==============================] - 78s 84ms/step - loss: 0.9485 - accuracy: 0.8000 - sparse_categorical_crossentropy: 1.5262 - val_loss: 0.6896 - val_accuracy: 0.8527 - val_sparse_categorical_crossentropy: 1.3110\n",
      "Epoch 2/6\n",
      "937/937 [==============================] - 76s 82ms/step - loss: 0.5635 - accuracy: 0.8824 - sparse_categorical_crossentropy: 1.2657 - val_loss: 0.5024 - val_accuracy: 0.8892 - val_sparse_categorical_crossentropy: 1.2297\n",
      "Epoch 3/6\n",
      "937/937 [==============================] - 76s 81ms/step - loss: 0.4517 - accuracy: 0.8971 - sparse_categorical_crossentropy: 1.2169 - val_loss: 0.4438 - val_accuracy: 0.8928 - val_sparse_categorical_crossentropy: 1.1814\n",
      "Epoch 4/6\n",
      "937/937 [==============================] - 76s 81ms/step - loss: 0.4077 - accuracy: 0.9015 - sparse_categorical_crossentropy: 1.1756 - val_loss: 0.4136 - val_accuracy: 0.8995 - val_sparse_categorical_crossentropy: 1.1607\n",
      "Epoch 5/6\n",
      "937/937 [==============================] - 76s 81ms/step - loss: 0.3850 - accuracy: 0.9051 - sparse_categorical_crossentropy: 1.1504 - val_loss: 0.3992 - val_accuracy: 0.8978 - val_sparse_categorical_crossentropy: 1.1329\n",
      "Epoch 6/6\n",
      "937/937 [==============================] - 76s 81ms/step - loss: 0.3719 - accuracy: 0.9093 - sparse_categorical_crossentropy: 1.1388 - val_loss: 0.4391 - val_accuracy: 0.8844 - val_sparse_categorical_crossentropy: 1.1219\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "H=model.fit(train_ds,epochs=6,\n",
    "            steps_per_epoch=np.floor(len(train_x)/batch_size).astype(np.int32),\n",
    "            validation_data=test_ds,\n",
    "            validation_steps=np.ceil(len(test_x)/batch_size).astype(np.int32),\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/test/saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "#分别保存两种格式的模型\n",
    "model.save(filepath=os.path.join(output_folder,'hdf5_model.h5'),save_format='h5')\n",
    "model.save(filepath=output_folder1,save_format='tf')\n",
    "#报的warning信息在tensorflow官网的例子中同样存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 model information...\n",
      "\n",
      "/tmp/test/hdf5_model\n",
      "└── hdf5_model.h5\n",
      "\n",
      "0 directories, 1 file\n",
      "19M\t/tmp/test/hdf5_model/hdf5_model.h5\n",
      "19M\t/tmp/test/hdf5_model\n",
      "saved model information...\n",
      "\n",
      "/tmp/test/saved_model\n",
      "├── assets\n",
      "├── saved_model.pb\n",
      "└── variables\n",
      "    ├── variables.data-00000-of-00002\n",
      "    ├── variables.data-00001-of-00002\n",
      "    └── variables.index\n",
      "\n",
      "2 directories, 4 files\n",
      "4.0K\t/tmp/test/saved_model/assets\n",
      "216K\t/tmp/test/saved_model/saved_model.pb\n",
      "19M\t/tmp/test/saved_model/variables/variables.data-00001-of-00002\n",
      "4.0K\t/tmp/test/saved_model/variables/variables.index\n",
      "8.0K\t/tmp/test/saved_model/variables/variables.data-00000-of-00002\n",
      "19M\t/tmp/test/saved_model/variables\n",
      "19M\t/tmp/test/saved_model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"hdf5 model information...\\n\"\n",
    "tree \"/tmp/test/hdf5_model\"\n",
    "\n",
    "du -ah \"/tmp/test/hdf5_model\"\n",
    "\n",
    "echo -e \"saved model information...\\n\"\n",
    "\n",
    "tree \"/tmp/test/saved_model\"\n",
    "\n",
    "du -ah \"/tmp/test/saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [-2.0793445e+00 -2.2612031e+00 -1.8440809e+00 -1.1460640e+00\n",
      " -1.9762940e+00  8.9537799e-03 -3.4592066e+00  3.3828874e+00\n",
      "  1.5507856e-01  9.2633562e+00]\n",
      "true label:9 pred label:9\n"
     ]
    }
   ],
   "source": [
    "#选取第一个样本来做为测试样本,用来评估不同模型之间的精度\n",
    "test_sample = train_x[0:1]\n",
    "test_y=train_y[0]\n",
    "out = model.predict(test_sample)\n",
    "print(\"probs:\",out[0])\n",
    "print(\"true label:{} pred label:{}\".format(test_y,np.argmax(out)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、各种模型间互转并验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 hdf5转saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf /tmp/test/hdf52saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5_model.h5\r\n"
     ]
    }
   ],
   "source": [
    "ls /tmp/test/hdf5_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/test/hdf52saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "hdf5_model = tf.keras.models.load_model(\"/tmp/test/hdf5_model/hdf5_model.h5\")\n",
    "hdf5_model.save(\"/tmp/test/hdf52saved_model\",save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 model information...\n",
      "\n",
      "/tmp/test/hdf5_model\n",
      "└── hdf5_model.h5\n",
      "\n",
      "0 directories, 1 file\n",
      "19M\t/tmp/test/hdf5_model/hdf5_model.h5\n",
      "19M\t/tmp/test/hdf5_model\n",
      "saved model information...\n",
      "\n",
      "/tmp/test/saved_model\n",
      "├── assets\n",
      "├── saved_model.pb\n",
      "└── variables\n",
      "    ├── variables.data-00000-of-00002\n",
      "    ├── variables.data-00001-of-00002\n",
      "    └── variables.index\n",
      "\n",
      "2 directories, 4 files\n",
      "4.0K\t/tmp/test/saved_model/assets\n",
      "216K\t/tmp/test/saved_model/saved_model.pb\n",
      "19M\t/tmp/test/saved_model/variables/variables.data-00001-of-00002\n",
      "4.0K\t/tmp/test/saved_model/variables/variables.index\n",
      "8.0K\t/tmp/test/saved_model/variables/variables.data-00000-of-00002\n",
      "19M\t/tmp/test/saved_model/variables\n",
      "19M\t/tmp/test/saved_model\n",
      "saved model information...\n",
      "\n",
      "/tmp/test/hdf52saved_model\n",
      "├── assets\n",
      "├── saved_model.pb\n",
      "└── variables\n",
      "    ├── variables.data-00000-of-00002\n",
      "    ├── variables.data-00001-of-00002\n",
      "    └── variables.index\n",
      "\n",
      "2 directories, 4 files\n",
      "4.0K\t/tmp/test/hdf52saved_model/assets\n",
      "224K\t/tmp/test/hdf52saved_model/saved_model.pb\n",
      "8.0K\t/tmp/test/hdf52saved_model/variables/variables.data-00001-of-00002\n",
      "4.0K\t/tmp/test/hdf52saved_model/variables/variables.index\n",
      "19M\t/tmp/test/hdf52saved_model/variables/variables.data-00000-of-00002\n",
      "19M\t/tmp/test/hdf52saved_model/variables\n",
      "19M\t/tmp/test/hdf52saved_model\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"hdf5 model information...\\n\"\n",
    "tree \"/tmp/test/hdf5_model\"\n",
    "\n",
    "du -ah \"/tmp/test/hdf5_model\"\n",
    "\n",
    "echo -e \"saved model information...\\n\"\n",
    "\n",
    "tree \"/tmp/test/saved_model\"\n",
    "\n",
    "du -ah \"/tmp/test/saved_model\"\n",
    "\n",
    "echo -e \"saved model information...\\n\"\n",
    "\n",
    "tree \"/tmp/test/hdf52saved_model\"\n",
    "\n",
    "du -ah \"/tmp/test/hdf52saved_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 saved model转hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-322a2edc9a6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/test/saved_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/test/hdf5_model/saved2hdf5_model.h5\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    973\u001b[0m     \"\"\"\n\u001b[1;32m    974\u001b[0m     saving.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m--> 975\u001b[0;31m                       signatures, options)\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    103\u001b[0m         not isinstance(model, sequential.Sequential)):\n\u001b[1;32m    104\u001b[0m       raise NotImplementedError(\n\u001b[0;32m--> 105\u001b[0;31m           \u001b[0;34m'Saving the model to HDF5 format requires the model to be a '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m           \u001b[0;34m'Functional model or a Sequential model. It does not work for '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m           \u001b[0;34m'subclassed models, because such models are defined via the body of '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Saving the model to HDF5 format requires the model to be a Functional model or a Sequential model. It does not work for subclassed models, because such models are defined via the body of a Python method, which isn't safely serializable. Consider saving to the Tensorflow SavedModel format (by setting save_format=\"tf\") or using `save_weights`."
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "saved_model = tf.keras.models.load_model(\"/tmp/test/saved_model\")\n",
    "saved_model.save(\"/tmp/test/hdf5_model/saved2hdf5_model.h5\",save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上信息说明，从saved model是无法转换成hdf5模型的，所以个人感觉在训练过程中保存hdf5格式的模型比较好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 所有模型精度测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试三个模型的精度，原始hdf5模型，原始saved model，hdf5转换的saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_hdf5_model = tf.keras.models.load_model(\"/tmp/test/hdf5_model/hdf5_model.h5\")\n",
    "origin_saved_model = tf.keras.models.load_model(\"/tmp/test/saved_model\")\n",
    "converted_saved_model = tf.keras.models.load_model(\"/tmp/test/hdf52saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [-2.0793445e+00 -2.2612031e+00 -1.8440809e+00 -1.1460640e+00\n",
      " -1.9762940e+00  8.9537799e-03 -3.4592066e+00  3.3828874e+00\n",
      "  1.5507856e-01  9.2633562e+00]\n",
      "true label:9 pred label:9\n",
      "probs: [-2.0793445e+00 -2.2612031e+00 -1.8440809e+00 -1.1460640e+00\n",
      " -1.9762940e+00  8.9537799e-03 -3.4592066e+00  3.3828874e+00\n",
      "  1.5507856e-01  9.2633562e+00]\n",
      "true label:9 pred label:9\n",
      "probs: [-2.0793445e+00 -2.2612031e+00 -1.8440809e+00 -1.1460640e+00\n",
      " -1.9762940e+00  8.9537799e-03 -3.4592066e+00  3.3828874e+00\n",
      "  1.5507856e-01  9.2633562e+00]\n",
      "true label:9 pred label:9\n"
     ]
    }
   ],
   "source": [
    "out1 = origin_hdf5_model.predict(test_sample)\n",
    "out2 = origin_saved_model.predict(test_sample)\n",
    "out3 = converted_saved_model.predict(test_sample)\n",
    "print(\"probs:\",out1[0])\n",
    "print(\"true label:{} pred label:{}\".format(test_y,np.argmax(out1)))\n",
    "print(\"probs:\",out2[0])\n",
    "print(\"true label:{} pred label:{}\".format(test_y,np.argmax(out2)))\n",
    "print(\"probs:\",out3[0])\n",
    "print(\"true label:{} pred label:{}\".format(test_y,np.argmax(out3)))\n",
    "np.testing.assert_array_almost_equal(out,out1)\n",
    "np.testing.assert_array_almost_equal(out,out2)\n",
    "np.testing.assert_array_almost_equal(out,out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到结果完全一致，模型转换没有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 hdf5和saved模型转tensorflow1.x pb模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们需要在tensorflow2.0中使用tensorflow1.x内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是hdf5转pb模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_names ['output_node/BiasAdd']\n",
      "len node1 626\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "##################################################################\n",
      "node: input_node\n",
      "node: conv2d/kernel\n",
      "node: conv2d/bias\n",
      "node: conv2d/Conv2D\n",
      "node: conv2d/BiasAdd\n",
      "node: conv2d/Relu\n",
      "node: max_pooling2d/MaxPool\n",
      "node: conv2d_1/kernel\n",
      "node: conv2d_1/bias\n",
      "node: conv2d_1/Conv2D\n",
      "node: conv2d_1/BiasAdd\n",
      "node: conv2d_1/Relu\n",
      "node: max_pooling2d_1/MaxPool\n",
      "node: flatten/Reshape/shape\n",
      "node: flatten/Reshape\n",
      "node: dense/kernel\n",
      "node: dense/bias\n",
      "node: dense/MatMul\n",
      "node: dense/BiasAdd\n",
      "node: dense/Relu\n",
      "node: output_node/kernel\n",
      "node: output_node/bias\n",
      "node: output_node/MatMul\n",
      "node: output_node/BiasAdd\n",
      "len node1 24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/test/pb_model/hdf52pb.pb'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.reset_default_graph()\n",
    "tf1.keras.backend.set_learning_phase(0) #调用模型前一定要执行该命令\n",
    "tf1.disable_v2_behavior() #禁止tensorflow2.0的行为\n",
    "#加载hdf5模型\n",
    "hdf5_pb_model = tf.keras.models.load_model(\"/tmp/test/hdf5_model/hdf5_model.h5\")\n",
    "def freeze_session(session,keep_var_names=None,output_names=None,clear_devices=True):\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "#         freeze_var_names = list(set(v.op.name for v in tf1.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "#         output_names += [v.op.name for v in tf1.global_variables()]\n",
    "        print(\"output_names\",output_names)\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "#         for node in input_graph_def.node:\n",
    "#             print('node:', node.name)\n",
    "        print(\"len node1\",len(input_graph_def.node))\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph =  tf1.graph_util.convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names)\n",
    "        \n",
    "        outgraph = tf1.graph_util.remove_training_nodes(frozen_graph)#云掉与推理无关的内容\n",
    "        print(\"##################################################################\")\n",
    "        for node in outgraph.node:\n",
    "            print('node:', node.name)\n",
    "        print(\"len node1\",len(outgraph.node))\n",
    "        return outgraph\n",
    "\n",
    "frozen_graph = freeze_session(tf1.keras.backend.get_session(),output_names=[out.op.name for out in hdf5_pb_model.outputs])\n",
    "tf1.train.write_graph(frozen_graph, output_folder2, \"hdf52pb.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是saved model转pb模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_names ['model/StatefulPartitionedCall']\n",
      "len node1 304\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "##################################################################\n",
      "node: conv2d/kernel\n",
      "node: conv2d/bias\n",
      "node: conv2d_1/kernel\n",
      "node: conv2d_1/bias\n",
      "node: dense/kernel\n",
      "node: dense/bias\n",
      "node: output_node/kernel\n",
      "node: output_node/bias\n",
      "node: input_1\n",
      "node: model/StatefulPartitionedCall\n",
      "len node1 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/test/pb_model/saved2pb.pb'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.reset_default_graph()\n",
    "tf1.keras.backend.set_learning_phase(0) #调用模型前一定要执行该命令\n",
    "tf1.disable_v2_behavior() #禁止tensorflow2.0的行为\n",
    "#加载hdf5模型\n",
    "saved_pb_model = tf.keras.models.load_model(\"/tmp/test/saved_model\")\n",
    "def freeze_session(session,keep_var_names=None,output_names=None,clear_devices=True):\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "#         freeze_var_names = list(set(v.op.name for v in tf1.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "#         output_names += [v.op.name for v in tf1.global_variables()]\n",
    "        print(\"output_names\",output_names)\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "#         for node in input_graph_def.node:\n",
    "#             print('node:', node.name)\n",
    "        print(\"len node1\",len(input_graph_def.node))\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph =  tf1.graph_util.convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names)\n",
    "        \n",
    "        outgraph = tf1.graph_util.remove_training_nodes(frozen_graph)#云掉与推理无关的内容\n",
    "        print(\"##################################################################\")\n",
    "        for node in outgraph.node:\n",
    "            print('node:', node.name)\n",
    "        print(\"len node1\",len(outgraph.node))\n",
    "        return outgraph\n",
    "\n",
    "frozen_graph = freeze_session(tf1.keras.backend.get_session(),output_names=[out.op.name for out in saved_pb_model.outputs])\n",
    "tf1.train.write_graph(frozen_graph, output_folder2, \"saved2pb.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是转换后的saved model转pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_names ['model/StatefulPartitionedCall']\n",
      "len node1 304\n",
      "INFO:tensorflow:Froze 8 variables.\n",
      "INFO:tensorflow:Converted 8 variables to const ops.\n",
      "##################################################################\n",
      "node: conv2d/kernel\n",
      "node: conv2d/bias\n",
      "node: conv2d_1/kernel\n",
      "node: conv2d_1/bias\n",
      "node: dense/kernel\n",
      "node: dense/bias\n",
      "node: output_node/kernel\n",
      "node: output_node/bias\n",
      "node: input_1\n",
      "node: model/StatefulPartitionedCall\n",
      "len node1 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/test/pb_model/hdf52saved2pb.pb'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "tf1.reset_default_graph()\n",
    "tf1.keras.backend.set_learning_phase(0) #调用模型前一定要执行该命令\n",
    "tf1.disable_v2_behavior() #禁止tensorflow2.0的行为\n",
    "#加载hdf5模型\n",
    "hdf52saved_pb_model = tf.keras.models.load_model(\"/tmp/test/hdf52saved_model/\")\n",
    "def freeze_session(session,keep_var_names=None,output_names=None,clear_devices=True):\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "#         freeze_var_names = list(set(v.op.name for v in tf1.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "#         output_names += [v.op.name for v in tf1.global_variables()]\n",
    "        print(\"output_names\",output_names)\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "#         for node in input_graph_def.node:\n",
    "#             print('node:', node.name)\n",
    "        print(\"len node1\",len(input_graph_def.node))\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph =  tf1.graph_util.convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names)\n",
    "        \n",
    "        outgraph = tf1.graph_util.remove_training_nodes(frozen_graph)#云掉与推理无关的内容\n",
    "        print(\"##################################################################\")\n",
    "        for node in outgraph.node:\n",
    "            print('node:', node.name)\n",
    "        print(\"len node1\",len(outgraph.node))\n",
    "        return outgraph\n",
    "\n",
    "frozen_graph = freeze_session(tf1.keras.backend.get_session(),output_names=[out.op.name for out in saved_pb_model.outputs])\n",
    "tf1.train.write_graph(frozen_graph, output_folder2, \"hdf52saved2pb.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/test\n",
      "├── hdf52saved_model\n",
      "│   ├── assets\n",
      "│   ├── saved_model.pb\n",
      "│   └── variables\n",
      "│       ├── variables.data-00000-of-00002\n",
      "│       ├── variables.data-00001-of-00002\n",
      "│       └── variables.index\n",
      "├── hdf5_model\n",
      "│   └── hdf5_model.h5\n",
      "├── pb_model\n",
      "│   ├── hdf52pb.pb\n",
      "│   ├── hdf52saved2pb.pb\n",
      "│   └── saved2pb.pb\n",
      "└── saved_model\n",
      "    ├── assets\n",
      "    ├── saved_model.pb\n",
      "    └── variables\n",
      "        ├── variables.data-00000-of-00002\n",
      "        ├── variables.data-00001-of-00002\n",
      "        └── variables.index\n",
      "\n",
      "8 directories, 12 files\n",
      "19M\t/tmp/test/hdf5_model/hdf5_model.h5\n",
      "19M\t/tmp/test/hdf5_model\n",
      "9.3M\t/tmp/test/pb_model/hdf52saved2pb.pb\n",
      "9.3M\t/tmp/test/pb_model/saved2pb.pb\n",
      "9.3M\t/tmp/test/pb_model/hdf52pb.pb\n",
      "28M\t/tmp/test/pb_model\n",
      "4.0K\t/tmp/test/saved_model/assets\n",
      "216K\t/tmp/test/saved_model/saved_model.pb\n",
      "19M\t/tmp/test/saved_model/variables/variables.data-00001-of-00002\n",
      "4.0K\t/tmp/test/saved_model/variables/variables.index\n",
      "8.0K\t/tmp/test/saved_model/variables/variables.data-00000-of-00002\n",
      "19M\t/tmp/test/saved_model/variables\n",
      "19M\t/tmp/test/saved_model\n",
      "4.0K\t/tmp/test/hdf52saved_model/assets\n",
      "224K\t/tmp/test/hdf52saved_model/saved_model.pb\n",
      "8.0K\t/tmp/test/hdf52saved_model/variables/variables.data-00001-of-00002\n",
      "4.0K\t/tmp/test/hdf52saved_model/variables/variables.index\n",
      "19M\t/tmp/test/hdf52saved_model/variables/variables.data-00000-of-00002\n",
      "19M\t/tmp/test/hdf52saved_model/variables\n",
      "19M\t/tmp/test/hdf52saved_model\n",
      "84M\t/tmp/test\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "tree /tmp/test\n",
    "\n",
    "du -ah /tmp/test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到三个pb模型的大小是相同的，但了节点名称不一样，且打印出来的名称顺序在hdf5模型体现更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 加载并测试pb模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三个pb模型分别进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf1\n",
    "import numpy as np\n",
    "\n",
    "def load_graph(file_path):\n",
    "    with tf1.gfile.GFile(file_path,'rb') as f:\n",
    "        graph_def = tf1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with tf1.Graph().as_default() as graph:\n",
    "        tf1.import_graph_def(graph_def,input_map = None,return_elements = None,name = \"\",op_dict = None,producer_op_list = None)\n",
    "    graph_nodes = [n for n in graph_def.node]\n",
    "    return graph,graph_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "三个模型依次调用：\n",
    "- /tmp/test/pb_model/hdf52saved2pb.pb\n",
    "- /tmp/test/pb_model/saved2pb.pb\n",
    "- /tmp/test/pb_model/hdf52pb.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第一个模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num nodes 24\n",
      "node: input_node\n",
      "node: conv2d/kernel\n",
      "node: conv2d/bias\n",
      "node: conv2d/Conv2D\n",
      "node: conv2d/BiasAdd\n",
      "node: conv2d/Relu\n",
      "node: max_pooling2d/MaxPool\n",
      "node: conv2d_1/kernel\n",
      "node: conv2d_1/bias\n",
      "node: conv2d_1/Conv2D\n",
      "node: conv2d_1/BiasAdd\n",
      "node: conv2d_1/Relu\n",
      "node: max_pooling2d_1/MaxPool\n",
      "node: flatten/Reshape/shape\n",
      "node: flatten/Reshape\n",
      "node: dense/kernel\n",
      "node: dense/bias\n",
      "node: dense/MatMul\n",
      "node: dense/BiasAdd\n",
      "node: dense/Relu\n",
      "node: output_node/kernel\n",
      "node: output_node/bias\n",
      "node: output_node/MatMul\n",
      "node: output_node/BiasAdd\n"
     ]
    }
   ],
   "source": [
    "file_path='/tmp/test/pb_model/hdf52pb.pb'\n",
    "graph,graph_nodes = load_graph(file_path)\n",
    "print(\"num nodes\",len(graph_nodes))\n",
    "for node in graph_nodes:\n",
    "    print('node:', node.name) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: [[-2.0793445e+00 -2.2612031e+00 -1.8440809e+00 -1.1460640e+00\n",
      "  -1.9762940e+00  8.9537799e-03 -3.4592066e+00  3.3828874e+00\n",
      "   1.5507856e-01  9.2633562e+00]]\n"
     ]
    }
   ],
   "source": [
    "input_node = graph.get_tensor_by_name('input_node:0')\n",
    "output = graph.get_tensor_by_name('output_node/BiasAdd:0')\n",
    "\n",
    "config = tf1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.25# 设定GPU使用占比\n",
    "config.gpu_options.visible_device_list = '0'  # '0,1'\n",
    "config.allow_soft_placement = True\n",
    "config.log_device_placement = False\n",
    "\n",
    "with tf1.Session(config=config,graph=graph) as sess:\n",
    "        logits = sess.run(output, feed_dict = {input_node:test_sample})\n",
    "print(\"logits:\",logits)\n",
    "np.testing.assert_array_almost_equal(out,logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从以上结果可以看到，hdf5转换的pb结果完全正确"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**第二个模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 1 of node model/StatefulPartitionedCall was passed float from conv2d/kernel:0 incompatible with expected resource.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    500\u001b[0m         results = c_api.TF_GraphImportGraphDefWithResults(\n\u001b[0;32m--> 501\u001b[0;31m             graph._c_graph, serialized, options)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScopedTFImportGraphDefResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Input 1 of node model/StatefulPartitionedCall was passed float from conv2d/kernel:0 incompatible with expected resource.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-4ad5ab9ba9bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/tmp/test/pb_model/saved2pb.pb'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"num nodes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'node:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-6c7963fc55a7>\u001b[0m in \u001b[0;36mload_graph\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mop_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproducer_op_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mgraph_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgraph_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 instructions)\n\u001b[0;32m--> 507\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36mimport_graph_def\u001b[0;34m(graph_def, input_map, return_elements, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m       \u001b[0mop_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m       producer_op_list=producer_op_list)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/framework/importer.py\u001b[0m in \u001b[0;36m_import_graph_def_internal\u001b[0;34m(graph_def, input_map, return_elements, validate_colocation_constraints, name, op_dict, producer_op_list)\u001b[0m\n\u001b[1;32m    503\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;31m# Create _DefinedFunctions for any imported functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 1 of node model/StatefulPartitionedCall was passed float from conv2d/kernel:0 incompatible with expected resource."
     ]
    }
   ],
   "source": [
    "file_path='/tmp/test/pb_model/saved2pb.pb'\n",
    "graph,graph_nodes = load_graph(file_path)\n",
    "print(\"num nodes\",len(graph_nodes))\n",
    "for node in graph_nodes:\n",
    "    print('node:', node.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出saved model转pb是可以的，但使用还是不可以使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**另外一种调用hdf5转换的pb(或在tensorflow2.x中调用tensorflow1.x转的pb)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf1.reset_default_graph()\n",
    "tf1.enable_v2_behavior()#tensorflow2.x中调用tensorflow1.x的内容需要激活tensorflow2.x的特性\n",
    "tf.keras.backend.clear_session()\n",
    "def wrap_frozen_graph(graph_def, inputs, outputs):\n",
    "    def _imports_graph_def():\n",
    "        tf1.import_graph_def(graph_def, name=\"\")\n",
    "    wrapped_import = tf1.wrap_function(_imports_graph_def, [])\n",
    "    import_graph = wrapped_import.graph\n",
    "    return wrapped_import.prune(\n",
    "        tf.nest.map_structure(import_graph.as_graph_element, inputs),\n",
    "        tf.nest.map_structure(import_graph.as_graph_element, outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node.name input_node\n",
      "node.name conv2d/kernel\n",
      "node.name conv2d/bias\n",
      "node.name conv2d/Conv2D\n",
      "node.name conv2d/BiasAdd\n",
      "node.name conv2d/Relu\n",
      "node.name max_pooling2d/MaxPool\n",
      "node.name conv2d_1/kernel\n",
      "node.name conv2d_1/bias\n",
      "node.name conv2d_1/Conv2D\n",
      "node.name conv2d_1/BiasAdd\n",
      "node.name conv2d_1/Relu\n",
      "node.name max_pooling2d_1/MaxPool\n",
      "node.name flatten/Reshape/shape\n",
      "node.name flatten/Reshape\n",
      "node.name dense/kernel\n",
      "node.name dense/bias\n",
      "node.name dense/MatMul\n",
      "node.name dense/BiasAdd\n",
      "node.name dense/Relu\n",
      "node.name output_node/kernel\n",
      "node.name output_node/bias\n",
      "node.name output_node/MatMul\n",
      "node.name output_node/BiasAdd\n"
     ]
    }
   ],
   "source": [
    "file_path='/tmp/test/pb_model/hdf52pb.pb'\n",
    "with open(file_path,'rb') as f:\n",
    "    graph_def = tf1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    for node in graph_def.node:\n",
    "        print(\"node.name\",node.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2.0793445e+00 -2.2612031e+00 -1.8440809e+00 -1.1460640e+00\n",
      "  -1.9762940e+00  8.9537799e-03 -3.4592066e+00  3.3828874e+00\n",
      "   1.5507856e-01  9.2633562e+00]], shape=(1, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model_func = wrap_frozen_graph(\n",
    "    graph_def, inputs='input_node:0',\n",
    "    outputs='output_node/BiasAdd:0')\n",
    "\n",
    "o=model_func(tf.constant(test_sample,dtype=tf.float32))\n",
    "\n",
    "print(o)\n",
    "\n",
    "np.testing.assert_array_almost_equal(out,o.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. tensorflow2.x保存的hdf5模型可以转tensorflow1.x的pb ,也可以转tensorflow2.x saved model\n",
    "2. saved model可以pb ,但是转换后无法使用\n",
    "3. 在tensorflow2.x中可以使用tensorflow1.x或tensorflow2.x的语法来调用\n",
    "所以我们在以后可以只保存hdf5模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
