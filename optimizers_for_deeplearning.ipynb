{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要参考内容[An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法是优化神经网络的最常用方法，在许多的深度学习框架中（比如：[tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers),[pytorch](https://pytorch.org/docs/stable/optim.html)等)都实现了多种优化算法。但是，这些算法通常用作黑盒优化器，因为很难对它们的优缺点进行实用的解释。本文首先看一下梯度下降法的变种，然后说明模型在训练时的困难所在。接着介绍最常用的优化算法以及他们解决模型训练难点上起的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法的目地是最小化目标函数$J(\\theta)$,通过对$\\theta$沿着目标函数$J(\\theta)$的梯度$\\nabla_{\\theta}J(\\theta)$反方向来更新。学习率$\\eta$决定更新步长大小。有关梯度下降法的更多说明[参见](https://cs231n.github.io/optimization-1/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
