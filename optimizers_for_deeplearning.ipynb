{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本文主要参考内容[An overview of gradient descent optimization algorithms](https://arxiv.org/pdf/1609.04747.pdf)以及https://ruder.io/optimizing-gradient-descent/index.html，  对文中优化算法用python代码实现，算法细节参照这些文档，本文更多侧重于代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法是优化神经网络的最常用方法，在许多的深度学习框架中（比如：[tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers),[pytorch](https://pytorch.org/docs/stable/optim.html)等)都实现了多种优化算法。但是，这些算法通常用作黑盒优化器，因为很难对它们的优缺点进行实用的解释。本文首先看一下梯度下降法的变种，然后说明模型在训练时的困难所在。接着介绍最常用的优化算法以及他们解决模型训练难点上起的作用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度下降法的目地是最小化目标函数$J(\\theta)$,通过对$\\theta$沿着目标函数$J(\\theta)$的梯度$\\nabla_{\\theta}J(\\theta)$反方向来更新。学习率$\\eta$决定更新步长大小。有关梯度下降法的更多说明[参见](https://cs231n.github.io/optimization-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1、梯度下降法变种"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有三种梯度下降法的变种，只是在计算目标函数梯度所用数据量不同的区别，数据量的大小可以可以影响梯度计算的准确度和参数更新的速度，数据越多，越准确，但是计算量会变大，会变慢，参有数更新也会变慢。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 批量梯度下降法 batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "是最初的梯度下降法，用训练集全部数据集来求梯度：$$\\theta = \\theta - \\eta * \\nabla_{\\theta}J(\\theta)$$ 由于用的数据太多，更新一次参数速度太慢，所以模型无法完成实时快速训练，但可以使在凸曲面上找到全局最优点，非凸曲面上的局部最优点。代码类似是：\n",
    "```python\n",
    "for i in range(n_epochs):\n",
    "    params_grad = compute_grads(objective_function,data,params)\n",
    "    params = params - lr*params_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 随机梯度下降法 stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用单个样本$(x_i,y_i)$来求梯度并更新参数：$$\\theta = \\theta - \\eta *\\nabla_{\\theta}J(\\theta;x^{(i)};y^{(i)}) $$用单个样本来更新，速度快，但是波动大，收敛到最小值后，容易再次退出。估码类似：\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  random.shuffle(data) #每个epoch都需要对数据进行打散\n",
    "  for example in data:\n",
    "    params_grad = compute_grads(objective_function, example, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 小批量梯度下降法 mini-batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "介于批量梯度下降法和随机梯度下降法之间，用训练数据中的部分数据来求梯度$$\\theta = \\theta - \\eta *\\nabla_{\\theta}J(\\theta;x^{(i:i+n)};y^{(i:i+n)}) $$ 好处理即快又稳定。代码类似：\n",
    "```python\n",
    "for i in range(nb_epochs):\n",
    "  random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=50):\n",
    "    params_grad = compute_grads(objective_function, batch, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 、难点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 很难选择合适的学习率，太小学习太慢，浪费时间； 太大无法收敛到最优处。\n",
    "- 学习率周期，使学习率按照提前设定或损失域值来改变学习率，但是这样无法适应训练集的特性。\n",
    "- 同一个学习率对所有参数进行更新也是不合适的。\n",
    "- 陷入非凸函数的局部最小值或拐点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、常用梯度优化算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加入动量，记录之前梯度方向的一部分：$$v_t = \\gamma*v_{t-1}+\\eta * \\nabla_{\\theta}J(\\theta)$$ $$\\theta = \\theta - v_t$$ $\\gamma$通常设置为0.9或类似。以下用python实现该算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent():\n",
    "    def __init__(self, learning_rate=0.01, momentum=0):\n",
    "        self.learning_rate = learning_rate \n",
    "        self.momentum = momentum\n",
    "        self.w_updt = None\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.w_updt is None:\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "        # Use momentum if set\n",
    "        self.w_updt = self.momentum * self.w_updt + (1 - self.momentum) * grad_wrt_w\n",
    "        # Move against the gradient to minimize loss\n",
    "        return w - self.learning_rate * self.w_updt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Nesterov 加速梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对梯度加了一个较正，能够对识差函数做一个更新适应，公式是：$$v_t = \\gamma*v_{t-1}+\\eta * \\nabla_{\\theta}J(\\theta-\\gamma*v_{t-1})$$ $$\\theta = \\theta - v_t$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NesterovAcceleratedGradient():\n",
    "    def __init__(self,learning_rate=0.001,momentum=0.4):\n",
    "        self.learning_rate=learning_rate\n",
    "        self.momentum=momentum\n",
    "        self.w_updt=np.array([])\n",
    "    def update(self,w,grad_func):\n",
    "         # Calculate the gradient of the loss a bit further down the slope from w\n",
    "        approx_future_grad = np.clip(grad_func(w - self.momentum * self.w_updt), -1, 1)\n",
    "        # Initialize on first update\n",
    "        if not self.w_updt.any():\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "\n",
    "        self.w_updt = self.momentum * self.w_updt + self.learning_rate * approx_future_grad\n",
    "        # Move against the gradient to minimize loss\n",
    "        return w - self.w_updt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Adagard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adagrad算法的一个主要优点是无需手动调整学习率。会对每一个参数做单独的调整。让学习率适应参数，对于出现次数较少的特征，我们对其采用更大的学习率，对于出现次数较多的特征，我们对其采用较小的学习率。因此，Adagrad非常适合处理稀疏数据。Adagrad的一个主要缺点是它在分母中累加梯度的平方：由于没增加一个正项，在整个训练过程中，累加的和会持续增长。这会导致学习率变小以至于最终变得无限小，在学习率无限小时，Adagrad算法将无法取得额外的信息。接下来的算法旨在解决这个不足。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对单个变量：$$ g_{t,i} = \\nabla_{\\theta}J(\\theta_i)$$ $$\\theta_{t+1,i} =\\theta_{t,i}-\\eta *g_{t,i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修正学习率：$$ g_{t,i} = \\nabla_{\\theta}J(\\theta_i)$$ $$\\theta_{t+1,i} =\\theta_{t,i}-{\\eta\\over{\\sqrt{G_{t,ii}}}} *g_{t,i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变为向量：$$\\theta_{t+1} =\\theta_{t}-{\\eta\\over{\\sqrt{G_{t}+\\epsilon}}}{\\bullet} g_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adagrad():\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.G = None # Sum of squares of the gradients\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.G is None:\n",
    "            self.G = np.zeros(np.shape(w))\n",
    "        # Add the square of the gradient of the loss function at w\n",
    "        self.G += np.power(grad_wrt_w, 2)\n",
    "        # Adaptive gradient with higher learning rate for sparse data\n",
    "        return w - self.learning_rate * grad_wrt_w / np.sqrt(self.G + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Adadelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adadelta是Adagrad的一种扩展算法，以处理Adagrad学习速率单调递减的问题。不是计算所有的梯度平方，Adadelta将计算计算历史梯度的窗口大小限制为一个固定值w。在Adadelta中，无需存储先前的w个平方梯度，而是将梯度的平方递归地表示成所有历史梯度平方的均值。在t时刻的均值$E[g^2]_t$只取决于先前的均值和当前的梯度（分量γ类似于动量项）："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[g^2]_{t+1}=\\gamma * E[g^2]_{t} + (1-\\gamma)*{g_t^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个更新：$$\\Delta{\\theta_t}=-\\eta \\bullet g_{t} $$ $$\\theta_{t+1}=\\theta_t +\\Delta{\\theta_t}$$ 先前adagrad更新算法：$$\\Delta{\\theta_{t}} =-{\\eta\\over{\\sqrt{G_{t}+\\epsilon}}}{\\bullet} g_{t}$$ 换掉$G_t$ $$\\Delta{\\theta_{t}} =-{\\eta\\over{\\sqrt{E[g_t^2]+\\epsilon}}}{\\bullet} g_{t}$$ 分母是一个均方根RMS:$$\\Delta{\\theta_{t}} =-{\\eta\\over {RMS[g]_t}}{\\bullet} g_{t}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adadelta():\n",
    "    def __init__(self, rho=0.95, eps=1e-6):\n",
    "        self.E_w_updt = None # Running average of squared parameter updates\n",
    "        self.E_grad = None   # Running average of the squared gradient of w\n",
    "        self.w_updt = None   # Parameter update\n",
    "        self.eps = eps\n",
    "        self.rho = rho\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.w_updt is None:\n",
    "            self.w_updt = np.zeros(np.shape(w))\n",
    "            self.E_w_updt = np.zeros(np.shape(w))\n",
    "            self.E_grad = np.zeros(np.shape(grad_wrt_w))\n",
    "\n",
    "        # Update average of gradients at w\n",
    "        self.E_grad = self.rho * self.E_grad + (1 - self.rho) * np.power(grad_wrt_w, 2)\n",
    "        \n",
    "        RMS_delta_w = np.sqrt(self.E_w_updt + self.eps)\n",
    "        RMS_grad = np.sqrt(self.E_grad + self.eps)\n",
    "\n",
    "        # Adaptive learning rate\n",
    "        adaptive_lr = RMS_delta_w / RMS_grad\n",
    "\n",
    "        # Calculate the update\n",
    "        self.w_updt = adaptive_lr * grad_wrt_w\n",
    "\n",
    "        # Update the running average of w updates\n",
    "        self.E_w_updt = self.rho * self.E_w_updt + (1 - self.rho) * np.power(self.w_updt, 2)\n",
    "\n",
    "        return w - self.w_updt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprob是adadelta的特例 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop():\n",
    "    def __init__(self, learning_rate=0.01, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.Eg = None # Running average of the square gradients at w\n",
    "        self.eps = 1e-8\n",
    "        self.rho = rho\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.Eg is None:\n",
    "            self.Eg = np.zeros(np.shape(grad_wrt_w))\n",
    "\n",
    "        self.Eg = self.rho * self.Eg + (1 - self.rho) * np.power(grad_wrt_w, 2)\n",
    "\n",
    "        # Divide the learning rate for a weight by a running average of the magnitudes of recent\n",
    "        # gradients for that weight\n",
    "        return w - self.learning_rate *  grad_wrt_w / np.sqrt(self.Eg + self.eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自适应动量估计算法，adaptive moment estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m_t = \\beta_1 m_{t-1}+(1-\\beta)g_t$$ $$v_t = \\beta_2 v_{t-1}+（1-\\beta_2)g_t^2$$ $$\\hat m_t ={ m_t \\over {1+\\beta_1^t}}$$$$\\hat v_t ={ v_t \\over {1+\\beta_2^t}}$$ $$\\theta_{t+1}=\\theta_t-{\\eta\\over{\\sqrt{\\hat v_t}+\\epsilon}}\\hat m_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    def __init__(self, learning_rate=0.001, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.eps = 1e-8\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        # Decay rates\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros(np.shape(grad_wrt_w))\n",
    "            self.v = np.zeros(np.shape(grad_wrt_w))\n",
    "        \n",
    "        self.m = self.b1 * self.m + (1 - self.b1) * grad_wrt_w\n",
    "        self.v = self.b2 * self.v + (1 - self.b2) * np.power(grad_wrt_w, 2)\n",
    "\n",
    "        m_hat = self.m / (1 - self.b1)\n",
    "        v_hat = self.v / (1 - self.b2)\n",
    "\n",
    "        self.w_updt = self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "        return w - self.w_updt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 AdaMax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k相比adam,只是改变一个公式， 中梯度平方用二范数，可以推广到任何范数：$$u_t = max(\\beta_2 v_{t-1},|g_t|）$$ $$\\theta_t = \\theta_{t-1}-{\\frac \\eta u_t}{\\hat{m_t}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaMax():\n",
    "    def __init__(self, learning_rate=0.002, b1=0.9, b2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        # Decay rates\n",
    "        self.b1 = b1\n",
    "        self.b2 = b2\n",
    "\n",
    "    def update(self, w, grad_wrt_w):\n",
    "        # If not initialized\n",
    "        if self.m is None:\n",
    "            self.m = np.zeros(np.shape(grad_wrt_w))\n",
    "            self.v = np.zeros(np.shape(grad_wrt_w))\n",
    "        \n",
    "        self.m = self.b1 * self.m + (1 - self.b1) * grad_wrt_w\n",
    "        self.u = max(self.b2 * self.v , np.abs(grad_wrt_w))\n",
    "\n",
    "        m_hat = self.m / (1 - self.b1)\n",
    "\n",
    "\n",
    "        self.w_updt = self.learning_rate * m_hat / self.u\n",
    "\n",
    "        return w - self.w_updt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam可以看成是momentum与RMSprop的结合，momentum是梯度的一阶，PMSprop是梯度的二阶，Nadam是结合adam与NAG算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum 算法：$$g_t = \\nabla_{\\theta_t} J(\\theta_t)$$ $$m_t = \\gamma m_{t-1}+\\eta g_t$$ $$\\theta_{t+1}=\\theta_t-m_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参第三个公式展开：$$\\theta_{t+1}=\\theta_t-(\\gamma m_{t-1}+\\eta g_t)$$ 可以看出更新方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAG算法：$$g_t = \\nabla_{\\theta_t} J(\\theta_t-{\\gamma m_{t-1}})$$ $$m_t = \\gamma m_{t-1}+\\eta g_t$$ $$\\theta_{t+1}=\\theta_t-m_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam算法：$$m_t = \\beta_1 m_{t-1}+(1-\\beta)g_t$$ $$\\hat m_t ={ m_t \\over {1+\\beta_1^t}}$$$$\\theta_{t+1}=\\theta_t-{\\eta\\over{\\sqrt{\\hat v_t}+\\epsilon}}\\hat m_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把第三个公式展开：$$\\theta_{t+1}=\\theta_t-{\\eta\\over{\\sqrt{\\hat v_t}+\\epsilon}}\n",
    "({{\\beta_1 m_{t-1}\\over{1-\\beta_1^t}}+{({1-\\beta_1 m_{t-1})g_t}\\over{1-\\beta_1^t}}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "括号中第一项可以看成$$\\beta_1 {\\hat m}_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的更新方法就是：$$\\theta_{t+1}=\\theta_t-{\\eta\\over{\\sqrt{\\hat v_t}+\\epsilon}}\n",
    "({{\\beta_1 {\\hat m}_{t-1}}+{({1-\\beta_1 m_{t-1})g_t}\\over{1-\\beta_1^t}}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 优化算法的选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自适应的算法用adam，或SGD momentum算法加学习率退火算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
